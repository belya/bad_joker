{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import itertools\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "from scipy.stats import norm\n",
    "import nltk.data\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import reuters\n",
    "from nltk. corpus import gutenberg\n",
    "from nltk.corpus import brown\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.layers import Input, Dense, Lambda, Layer, LSTM, Reshape, TimeDistributed, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing code is data specific.  \n",
    "  \n",
    "It is an example of how one can use a pre-trained word2vec to embed sentences into a vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO train w2v model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"jokes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = (dataset[\"Question\"] + \" \" + dataset[\"Answer\"]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"shortjokes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences += dataset[\"Joke\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    return re.sub(\n",
    "        r\"[^\\w\\s]\", \n",
    "        \"\", \n",
    "        text\n",
    "    ).lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_sentences = [preprocess_text(t) for t in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "269926"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(preprocessed_sentences, size=100, window=10, workers=4, iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('prostitute', 0.8248806595802307),\n",
       " ('crabs', 0.5551421642303467),\n",
       " ('hookers', 0.5246987342834473),\n",
       " ('prostitutes', 0.5014286637306213),\n",
       " ('chick', 0.4723253548145294),\n",
       " ('lobster', 0.46344321966171265),\n",
       " ('woman', 0.4505305588245392),\n",
       " ('prostitue', 0.44164422154426575),\n",
       " ('rooster', 0.4328981935977936),\n",
       " ('whore', 0.4174628257751465)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"hooker\"], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = w2v_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sentence(sentence):\n",
    "    concat_vector = []\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            concat_vector.append(w2v[word])\n",
    "        except:\n",
    "            pass\n",
    "    return [a for vector in concat_vector for a in vector]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing text from a variety of different sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_sentences = [vectorize_sentence(sentence) for sentence in preprocessed_sentences if len(sentence) < 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110440"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dim = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "vectorized_padded = sequence.pad_sequences(vectorized_sentences, maxlen=original_dim, padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to shuffle the text vectors before splitting them into test and train samples.   \n",
    "  \n",
    "This is done to avoid clumping text with similar context and style in the dataset because it can confuse the neural network during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_train, vectorized_test = train_test_split(vectorized_padded, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "def cut_dataset(dataset, batch_size):\n",
    "    rest = len(dataset) % batch_size\n",
    "    return dataset[:-rest]\n",
    "\n",
    "vectorized_train = cut_dataset(vectorized_train, batch_size)\n",
    "vectorized_test = cut_dataset(vectorized_test, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get w2v embeddings for text with fixed length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 1000\n",
    "intermediate_dim = 1200\n",
    "lstm_intermediate_dim = 100\n",
    "epochs = 200\n",
    "epsilon_std = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Input(shape=(original_dim,))\n",
    "h = Dense(intermediate_dim, activation='sigmoid')(x)\n",
    "z_mean = Dense(latent_dim)(h)\n",
    "z_log_var = Dense(latent_dim)(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0.,\n",
    "                              stddev=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we instantiate these layers separately so as to reuse them later\n",
    "decoder_h = Dense(intermediate_dim, activation='sigmoid')\n",
    "decoder_mean = Dense(original_dim, activation='sigmoid')\n",
    "h_decoded = decoder_h(z)\n",
    "x_decoded_mean = decoder_mean(h_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(y_true, y_pred):\n",
    "    xent_loss = K.sum(K.binary_crossentropy(y_pred, y_true), axis=-1)\n",
    "    kl_loss = 0.5 * K.sum(K.square(z_mean) + K.exp(z_log_var) - 1. - z_log_var, axis=-1)\n",
    "    return xent_loss + kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = Model(x, x_decoded_mean)\n",
    "vae.compile(optimizer='adam', loss=vae_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 77200 samples, validate on 33000 samples\n",
      "Epoch 1/200\n",
      "77200/77200 [==============================] - 76s 987us/step - loss: 5418.0078 - val_loss: 5229.0336\n",
      "\n",
      "Epoch 00001: saving model to /tmp/model.h5\n",
      "Epoch 2/200\n",
      "77200/77200 [==============================] - 70s 906us/step - loss: 5181.8310 - val_loss: 5129.7372\n",
      "\n",
      "Epoch 00002: saving model to /tmp/model.h5\n",
      "Epoch 3/200\n",
      "77200/77200 [==============================] - 89s 1ms/step - loss: 5104.9092 - val_loss: 5094.6953\n",
      "\n",
      "Epoch 00003: saving model to /tmp/model.h5\n",
      "Epoch 4/200\n",
      "77200/77200 [==============================] - 90s 1ms/step - loss: 5074.8691 - val_loss: 5061.4223\n",
      "\n",
      "Epoch 00004: saving model to /tmp/model.h5\n",
      "Epoch 5/200\n",
      "77200/77200 [==============================] - 85s 1ms/step - loss: 5046.9688 - val_loss: 5035.2532\n",
      "\n",
      "Epoch 00005: saving model to /tmp/model.h5\n",
      "Epoch 6/200\n",
      "77200/77200 [==============================] - 96s 1ms/step - loss: 5021.2118 - val_loss: 5017.1991\n",
      "\n",
      "Epoch 00006: saving model to /tmp/model.h5\n",
      "Epoch 7/200\n",
      "77200/77200 [==============================] - 88s 1ms/step - loss: 5003.2236 - val_loss: 5000.2458\n",
      "\n",
      "Epoch 00007: saving model to /tmp/model.h5\n",
      "Epoch 8/200\n",
      "77200/77200 [==============================] - 88s 1ms/step - loss: 4988.8142 - val_loss: 4988.8022\n",
      "\n",
      "Epoch 00008: saving model to /tmp/model.h5\n",
      "Epoch 9/200\n",
      "77200/77200 [==============================] - 87s 1ms/step - loss: 4978.8097 - val_loss: 4978.5438\n",
      "\n",
      "Epoch 00009: saving model to /tmp/model.h5\n",
      "Epoch 10/200\n",
      "77200/77200 [==============================] - 86s 1ms/step - loss: 4969.7660 - val_loss: 4972.3812\n",
      "\n",
      "Epoch 00010: saving model to /tmp/model.h5\n",
      "Epoch 11/200\n",
      "77200/77200 [==============================] - 87s 1ms/step - loss: 4963.0351 - val_loss: 4964.9924\n",
      "\n",
      "Epoch 00011: saving model to /tmp/model.h5\n",
      "Epoch 12/200\n",
      "77200/77200 [==============================] - 88s 1ms/step - loss: 4949.4623 - val_loss: 4945.2546\n",
      "\n",
      "Epoch 00012: saving model to /tmp/model.h5\n",
      "Epoch 13/200\n",
      "77200/77200 [==============================] - 86s 1ms/step - loss: 4925.5809 - val_loss: 4920.4714\n",
      "\n",
      "Epoch 00013: saving model to /tmp/model.h5\n",
      "Epoch 14/200\n",
      "77200/77200 [==============================] - 82s 1ms/step - loss: 4892.3743 - val_loss: 4880.2748\n",
      "\n",
      "Epoch 00014: saving model to /tmp/model.h5\n",
      "Epoch 15/200\n",
      "77200/77200 [==============================] - 80s 1ms/step - loss: 4861.1422 - val_loss: 4856.2988\n",
      "\n",
      "Epoch 00015: saving model to /tmp/model.h5\n",
      "Epoch 16/200\n",
      "77200/77200 [==============================] - 78s 1ms/step - loss: 4837.0948 - val_loss: 4835.3753\n",
      "\n",
      "Epoch 00016: saving model to /tmp/model.h5\n",
      "Epoch 17/200\n",
      "77200/77200 [==============================] - 78s 1ms/step - loss: 4812.8228 - val_loss: 4808.6714\n",
      "\n",
      "Epoch 00017: saving model to /tmp/model.h5\n",
      "Epoch 18/200\n",
      "77200/77200 [==============================] - 86s 1ms/step - loss: 4782.5404 - val_loss: 4776.3167\n",
      "\n",
      "Epoch 00018: saving model to /tmp/model.h5\n",
      "Epoch 19/200\n",
      "77200/77200 [==============================] - 82s 1ms/step - loss: 4755.9872 - val_loss: 4757.6965\n",
      "\n",
      "Epoch 00019: saving model to /tmp/model.h5\n",
      "Epoch 20/200\n",
      "77200/77200 [==============================] - 76s 981us/step - loss: 4734.5125 - val_loss: 4737.0562\n",
      "\n",
      "Epoch 00020: saving model to /tmp/model.h5\n",
      "Epoch 21/200\n",
      "77200/77200 [==============================] - 83s 1ms/step - loss: 4717.2811 - val_loss: 4722.7636\n",
      "\n",
      "Epoch 00021: saving model to /tmp/model.h5\n",
      "Epoch 22/200\n",
      "77200/77200 [==============================] - 86s 1ms/step - loss: 4700.2609 - val_loss: 4706.4149\n",
      "\n",
      "Epoch 00022: saving model to /tmp/model.h5\n",
      "Epoch 23/200\n",
      "77200/77200 [==============================] - 86s 1ms/step - loss: 4686.8208 - val_loss: 4696.3595\n",
      "\n",
      "Epoch 00023: saving model to /tmp/model.h5\n",
      "Epoch 24/200\n",
      "77200/77200 [==============================] - 86s 1ms/step - loss: 4673.9687 - val_loss: 4684.1600\n",
      "\n",
      "Epoch 00024: saving model to /tmp/model.h5\n",
      "Epoch 25/200\n",
      "77200/77200 [==============================] - 96s 1ms/step - loss: 4659.3987 - val_loss: 4667.6261\n",
      "\n",
      "Epoch 00025: saving model to /tmp/model.h5\n",
      "Epoch 26/200\n",
      "77200/77200 [==============================] - 85s 1ms/step - loss: 4643.9311 - val_loss: 4654.8866\n",
      "\n",
      "Epoch 00026: saving model to /tmp/model.h5\n",
      "Epoch 27/200\n",
      "77200/77200 [==============================] - 102s 1ms/step - loss: 4632.0824 - val_loss: 4644.4228\n",
      "\n",
      "Epoch 00027: saving model to /tmp/model.h5\n",
      "Epoch 28/200\n",
      "77200/77200 [==============================] - 105s 1ms/step - loss: 4620.3888 - val_loss: 4633.3226\n",
      "\n",
      "Epoch 00028: saving model to /tmp/model.h5\n",
      "Epoch 29/200\n",
      "77200/77200 [==============================] - 86s 1ms/step - loss: 4607.0520 - val_loss: 4618.1989\n",
      "\n",
      "Epoch 00029: saving model to /tmp/model.h5\n",
      "Epoch 30/200\n",
      "77200/77200 [==============================] - 85s 1ms/step - loss: 4593.4129 - val_loss: 4608.7323\n",
      "\n",
      "Epoch 00030: saving model to /tmp/model.h5\n",
      "Epoch 31/200\n",
      "77200/77200 [==============================] - 82s 1ms/step - loss: 4580.1811 - val_loss: 4592.5597\n",
      "\n",
      "Epoch 00031: saving model to /tmp/model.h5\n",
      "Epoch 32/200\n",
      "77200/77200 [==============================] - 83s 1ms/step - loss: 4564.6768 - val_loss: 4580.0917\n",
      "\n",
      "Epoch 00032: saving model to /tmp/model.h5\n",
      "Epoch 33/200\n",
      "77200/77200 [==============================] - 76s 987us/step - loss: 4552.4973 - val_loss: 4570.6110\n",
      "\n",
      "Epoch 00033: saving model to /tmp/model.h5\n",
      "Epoch 34/200\n",
      "77200/77200 [==============================] - 80s 1ms/step - loss: 4542.5597 - val_loss: 4561.8950\n",
      "\n",
      "Epoch 00034: saving model to /tmp/model.h5\n",
      "Epoch 35/200\n",
      "77200/77200 [==============================] - 74s 961us/step - loss: 4532.7676 - val_loss: 4552.3993\n",
      "\n",
      "Epoch 00035: saving model to /tmp/model.h5\n",
      "Epoch 36/200\n",
      "77200/77200 [==============================] - 80s 1ms/step - loss: 4520.8792 - val_loss: 4541.4896\n",
      "\n",
      "Epoch 00036: saving model to /tmp/model.h5\n",
      "Epoch 37/200\n",
      "77200/77200 [==============================] - 74s 964us/step - loss: 4508.0373 - val_loss: 4529.2416\n",
      "\n",
      "Epoch 00037: saving model to /tmp/model.h5\n",
      "Epoch 38/200\n",
      "77200/77200 [==============================] - 76s 979us/step - loss: 4497.9497 - val_loss: 4521.6429\n",
      "\n",
      "Epoch 00038: saving model to /tmp/model.h5\n",
      "Epoch 39/200\n",
      "77200/77200 [==============================] - 85s 1ms/step - loss: 4488.1252 - val_loss: 4509.1906\n",
      "\n",
      "Epoch 00039: saving model to /tmp/model.h5\n",
      "Epoch 40/200\n",
      "77200/77200 [==============================] - 80s 1ms/step - loss: 4475.7101 - val_loss: 4499.9292\n",
      "\n",
      "Epoch 00040: saving model to /tmp/model.h5\n",
      "Epoch 41/200\n",
      "77200/77200 [==============================] - 78s 1ms/step - loss: 4464.1980 - val_loss: 4489.6041\n",
      "\n",
      "Epoch 00041: saving model to /tmp/model.h5\n",
      "Epoch 42/200\n",
      "77200/77200 [==============================] - 80s 1ms/step - loss: 4455.7395 - val_loss: 4482.6681\n",
      "\n",
      "Epoch 00042: saving model to /tmp/model.h5\n",
      "Epoch 43/200\n",
      "77200/77200 [==============================] - 79s 1ms/step - loss: 4448.4651 - val_loss: 4475.6978\n",
      "\n",
      "Epoch 00043: saving model to /tmp/model.h5\n",
      "Epoch 44/200\n",
      "77200/77200 [==============================] - 83s 1ms/step - loss: 4439.3468 - val_loss: 4466.3437\n",
      "\n",
      "Epoch 00044: saving model to /tmp/model.h5\n",
      "Epoch 45/200\n",
      "19200/77200 [======>.......................] - ETA: 1:02 - loss: 4424.0784"
     ]
    }
   ],
   "source": [
    "#checkpoint\n",
    "cp = [callbacks.ModelCheckpoint(filepath=\"/tmp/model.h5\", verbose=1)]\n",
    "\n",
    "#train\n",
    "vae.fit(vectorized_train, vectorized_train,\n",
    "        shuffle=True,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(vectorized_test, vectorized_test),\n",
    "        callbacks=cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a model to project inputs on the latent space\n",
    "encoder = Model(x, z_mean)\n",
    "\n",
    "# build a generator that can sample from the learned distribution\n",
    "decoder_input = Input(shape=(latent_dim,))\n",
    "_h_decoded = decoder_h(decoder_input)\n",
    "_x_decoded_mean = decoder_mean(_h_decoded)\n",
    "generator = Model(decoder_input, _x_decoded_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Text From Latent Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some matrix magic\n",
    "def sent_parse(sentence, mat_shape):\n",
    "    data_concat = []\n",
    "    word_vecs = vectorize_sentences(sentence)\n",
    "    for x in word_vecs:\n",
    "        data_concat.append(list(itertools.chain.from_iterable(x)))\n",
    "    zero_matr = np.zeros(mat_shape)\n",
    "    zero_matr[0] = np.array(data_concat)\n",
    "    return zero_matr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sentence_with_w2v(sent_vect):\n",
    "    word_sent = ''\n",
    "    tocut = sent_vect\n",
    "    for i in range (int(len(sent_vect)/100)):\n",
    "        word_sent += w2v.most_similar(positive=[tocut[:100]], topn=1)[0][0]\n",
    "        word_sent += ' '\n",
    "        tocut = tocut[100:]\n",
    "    print(word_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input: encoded sentence vector\n",
    "# output: encoded sentence vector in dataset with highest cosine similarity\n",
    "def find_similar_encoding(sent_vect):\n",
    "    all_cosine = []\n",
    "    for sent in sent_encoded:\n",
    "        result = 1 - spatial.distance.cosine(sent_vect, sent)\n",
    "        all_cosine.append(result)\n",
    "    data_array = np.array(all_cosine)\n",
    "    maximum = data_array.argsort()[-3:][::-1][1]\n",
    "    new_vec = sent_encoded[maximum]\n",
    "    return new_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input: two points, integer n\n",
    "# output: n equidistant points on the line between the input points (inclusive)\n",
    "def shortest_homology(point_one, point_two, num):\n",
    "    dist_vec = point_two - point_one\n",
    "    sample = np.linspace(0, 1, num, endpoint = True)\n",
    "    hom_sample = []\n",
    "    for s in sample:\n",
    "        hom_sample.append(point_one + s * dist_vec)\n",
    "    return hom_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input: two written sentences, VAE batch-size, dimension of VAE input\n",
    "# output: the function embeds the sentences in latent-space, and then prints their generated text representations\n",
    "# along with the text representations of several points in between them\n",
    "def sent_2_sent(sent1,sent2, batch, dim):\n",
    "    a = sent_parse([sent1], (batch,dim))\n",
    "    b = sent_parse([sent2], (batch,dim))\n",
    "    encode_a = encoder.predict(a, batch_size = batch)\n",
    "    encode_b = encoder.predict(b, batch_size = batch)\n",
    "    test_hom = hom_shortest(encode_a[0], encode_b[0], 5)\n",
    "    \n",
    "    for point in test_hom:\n",
    "        p = generator.predict(np.array([point]))[0]\n",
    "        print_sentence(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing sentences from the training set and comparing them with the original will test whether the custom print function works properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', 'nearly', 'drown', 'in', 'his', 'own', 'tea', 'pee']\n",
      "he nearly drown in his own tea pee the the the the the the the the the the the the \n",
      "['mycheexarphlexin']\n",
      "the the the the the the the the the the the the the the the the the the the the \n",
      "['matt']\n",
      "matt the the the the the the the the the the the the the the the the the the the \n",
      "['jeanluc', 'pickacard']\n",
      "the the the the the the the the the the the the the the the the the the the the \n",
      "['a', 'bullet', 'doesnt', 'miss', 'harambe']\n",
      "a bullet doesnt miss harambe the the the the the the the the the the the the the the the \n",
      "['he', 'was', 'having', 'a', 'midlife', 'crisis']\n",
      "he was having a midlife crisis the the the the the the the the the the the the the the \n",
      "['one', 'shucks', 'between', 'fits']\n",
      "one shucks between fits the the the the the the the the the the the the the the the the \n",
      "['kevin', 'durant', 'or', 'bernie', 'sanders']\n",
      "kevin or bernie sanders the the the the the the the the the the the the the the the the \n",
      "['because', 'the', 'shark', 'burped']\n",
      "because the shark the the the the the the the the the the the the the the the the the \n",
      "['a', 'bachelor', 'will', 'go', 'to', 'the', 'fridge', 'sees', 'nothing', 'he', 'wants', 'and', 'go', 'to', 'bed', 'a', 'married', 'man', 'will', 'go', 'the', 'bed', 'sees', 'nothing', 'he', 'wants', 'and', 'go', 'the', 'fridge']\n",
      "a will go to the fridge sees nothing he wants and go to bed a married man will go the \n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 10):\n",
    "    print(preprocessed_x_sentences[i])\n",
    "    print_sentence_with_w2v(x_vectorized_padded[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder takes the training set of sentence vectors (concatenanted word vectors) and embeds them into a lower dimensional vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_encoded = encoder.predict(x_vectorized_padded[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder takes the list of latent dimensional encodings from above and turns them back into vectors of their original dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_decoded = generator.predict(sent_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brides equal purchase manage purchase purchase purchase purchase purchase um purchase purchase purchase purchase purchase purchase purchase purchase purchase purchase \n"
     ]
    }
   ],
   "source": [
    "print_sentence_with_w2v(sent_decoded[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder trained above embeds sentences (concatenated word vetors) into a lower dimensional space. The code below takes two of these lower dimensional sentence representations and finds five points between them. It then uses the trained decoder to project these five points into the higher, original, dimensional space. Finally, it reveals the text represented by the five generated sentence vectors by taking each word vector concatenated inside and finding the text associated with it in the word2vec used during preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_hom = shortest_homology(sent_encoded[3], sent_encoded[10], 5)\n",
    "for point in test_hom:\n",
    "    p = generator.predict(np.array([point]))[0]\n",
    "    print_sentence_with_w2v(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below does the same thing, with one important difference. After sampling equidistant points in the latent space between two sentence embeddings, it finds the embeddings from our encoded dataset those points are most similar to. It then prints the text associated with those vectors.\n",
    "  \n",
    "This allows us to explore how the Variational Autoencoder clusters our dataset of sentences in latent space. It lets us investigate whether sentences with similar concepts or grammatical styles are represented in similar areas of the lower dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_hom = shortest_homology(sent_encoded[2], sent_encoded[1500], 20)\n",
    "for point in test_hom:\n",
    "    p = generator.predict(np.array([find_similar_encoding(point)]))[0]\n",
    "    print_sentence_with_w2v(p)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
