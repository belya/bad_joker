{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import itertools\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "from scipy.stats import norm\n",
    "import nltk.data\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import reuters\n",
    "from nltk. corpus import gutenberg\n",
    "from nltk.corpus import brown\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.layers import Input, Dense, Lambda, Layer, LSTM, Reshape, TimeDistributed, Dropout, Bidirectional, RepeatVector, Flatten\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Model, Sequential\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "import gensim.downloader as api\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.test.utils import common_texts, get_tmpfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing code is data specific.  \n",
    "  \n",
    "It is an example of how one can use a pre-trained word2vec to embed sentences into a vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO train w2v model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = api.load('20-newsgroups')\n",
    "# info = api.info()  # show info about available models/datasets\n",
    "# word2vec_model = api.load(\"glove-twitter-100\")  # download the model and return as object ready for use\n",
    "# word2vec_model.most_similar(\"cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1200,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"shortjokes.csv\").set_index(\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1 = pd.read_csv(\"jokes_score_name_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1[\"Joke\"] = dataset_1[\"q\"] + \" \" + dataset_1[\"a\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_2 = pd.read_csv(\"qajokes1.1.2.csv\")\n",
    "dataset_2[\"Joke\"] = dataset_2[\"Question\"] + \" \" + dataset_2[\"Answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = dataset[\"Joke\"].tolist() + dataset_1[\"Joke\"].tolist() + dataset_2[\"Joke\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    return re.sub(\n",
    "        r\"[^\\w\\s']\", \n",
    "        \"\", \n",
    "        text\n",
    "    ).lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_sentences = [preprocess_text(t) for t in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "440099"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v_model = Word2Vec(preprocessed_sentences, size=100, window=10, workers=16, iter=100)\n",
    "# w2v_model = word2vec_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('phelps', 0.5148839950561523),\n",
       " (\"bolt's\", 0.5110259056091309),\n",
       " ('bloodbath', 0.45941269397735596),\n",
       " ('gust', 0.4141930341720581),\n",
       " ('nada', 0.4051152169704437),\n",
       " ('glimpse', 0.40208110213279724),\n",
       " ('key', 0.3946117162704468),\n",
       " ('ricocheted', 0.39381635189056396),\n",
       " ('lightning', 0.38503432273864746),\n",
       " ('binos', 0.38127273321151733)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(\"bolt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = w2v_model.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing text from a variety of different sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 45000., 233729., 105550.,  20059.,   4429.,   2733.,   2216.,\n",
       "          2010.,   1910.,   1746.]),\n",
       " array([  1. ,  10.9,  20.8,  30.7,  40.6,  50.5,  60.4,  70.3,  80.2,\n",
       "         90.1, 100. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 632,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([len(s) + 1 for s in preprocessed_sentences if len(s) < 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No LSTM variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessed_sentences = [preprocess_text(t) for t in dataset_2[\"Joke\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dim = 2000\n",
    "original_dims = (2000, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sentence(sentence):\n",
    "    concat_vector = []\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            concat_vector.append(w2v[word])\n",
    "        except:\n",
    "            return\n",
    "    return [a for vector in concat_vector for a in vector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_sentences = [vectorize_sentence(sentence) for sentence in preprocessed_sentences if (len(sentence) < 20) & (len(sentence) > 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_sentences = [sentence for sentence in vectorized_sentences if sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "vectorized_padded = sequence.pad_sequences(vectorized_sentences, maxlen=original_dim, padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized_vectorized_padded = (vectorized_padded - vectorized_padded.min()) / (vectorized_padded.max() - vectorized_padded.min())# + 0.01*np.random.randn(*normalized_vectorized_padded.shape)\n",
    "normalized_vectorized_padded = vectorized_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_sentences = [preprocess_text(t) for t in dataset_2[\"Joke\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sentence(sentence):\n",
    "    concat_vector = []\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            concat_vector.append(w2v[word])\n",
    "        except:\n",
    "            return\n",
    "    return concat_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_sentences = [vectorize_sentence(sentence) for sentence in preprocessed_sentences if (len(sentence) < 20) & (len(sentence) > 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_sentences = [sentence for sentence in vectorized_sentences if sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_padded = np.array([sentence + [[0] * 100] * (20 - len(sentence)) for sentence in vectorized_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_vectorized_padded = (vectorized_padded - vectorized_padded.min()) / (vectorized_padded.max() - vectorized_padded.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dims = (20, 100)\n",
    "original_dim = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM tokens variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_sentences = [preprocess_text(t) for t in dataset_2[\"Joke\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary():\n",
    "    pass\n",
    "\n",
    "def vectorize_sentence(sentence):\n",
    "    concat_vector = []\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            concat_vector.append(w2v[word])\n",
    "        except:\n",
    "            return\n",
    "    return concat_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_sentences = [vectorize_sentence(sentence) for sentence in preprocessed_sentences if (len(sentence) < 20) & (len(sentence) > 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_sentences = [sentence for sentence in vectorized_sentences if sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_padded = np.array([sentence + [[0] * 100] * (20 - len(sentence)) for sentence in vectorized_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_vectorized_padded = (vectorized_padded - vectorized_padded.min()) / (vectorized_padded.max() - vectorized_padded.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dims = (20, 100)\n",
    "original_dim = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to shuffle the text vectors before splitting them into test and train samples.   \n",
    "  \n",
    "This is done to avoid clumping text with similar context and style in the dataset because it can confuse the neural network during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_train, vectorized_test = train_test_split(normalized_vectorized_padded, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 500\n",
    "def cut_dataset(dataset, batch_size):\n",
    "    rest = len(dataset) % batch_size\n",
    "    return dataset[:-rest]\n",
    "\n",
    "vectorized_train = cut_dataset(vectorized_train, batch_size)\n",
    "vectorized_test = cut_dataset(vectorized_test, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [],
   "source": [
    "help_vectorized_train = vectorized_train.reshape(-1, original_dim)\n",
    "help_vectorized_test = vectorized_test.reshape(-1, original_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get w2v embeddings for text with fixed length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Values are normalized for sigmoid - done\n",
    "2. LSTM usage in encoder - done\n",
    "3. LSTM usage in decoder - done (without Flatten - not done)\n",
    "4. Layers dropout - not done, underfitting\n",
    "5. Tokens instead of word2vec vectors? Or use something instead of binary crossentropy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary crossentropy for non binary values?!\n",
    "\n",
    "Use something that can replace E[log P(X|z)]\n",
    "\n",
    "Why cross-entropy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 100\n",
    "lstm_dim = 100\n",
    "intermediate_dim = 1000\n",
    "epochs = 200\n",
    "epsilon_std = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Input(shape=original_dims)\n",
    "h = Dense(intermediate_dim, activation='relu')(x)\n",
    "h_mean = Dense(latent_dim, activation='relu')(h)\n",
    "z_mean = Dense(latent_dim)(h_mean)\n",
    "h_log_var = Dense(latent_dim, activation='relu')(h)\n",
    "z_log_var = Dense(latent_dim)(h_log_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0.,\n",
    "                              stddev=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we instantiate these layers separately so as to reuse them later\n",
    "decoder_h = Dense(intermediate_dim, activation='relu')\n",
    "decoder_mean = Dense(original_dim, activation='linear') \n",
    "\n",
    "h_decoded = decoder_h(z)\n",
    "x_decoded_mean = decoder_mean(h_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def xent_loss(y_true, y_pred):\n",
    "# #     return K.sum(K.binary_crossentropy(y_true, y_pred), axis=1)\n",
    "#     return original_dim * K.binary_crossentropy(y_true, y_pred)\n",
    "\n",
    "# def kl_loss(y_true, y_pred):\n",
    "# #     return 0.5 * K.sum(K.square(z_mean) + K.exp(z_log_var) - 1. - z_log_var, axis=1)\n",
    "#     return - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "\n",
    "# def vae_loss(y_true, y_pred):\n",
    "# #     return xent_loss(y_true, y_pred) + kl_loss(y_true, y_pred)\n",
    "#     return K.mean(xent_loss(y_true, y_pred) + kl_loss(y_true, y_pred))\n",
    "\n",
    "def vae_loss(x, x_decoded_mean):\n",
    "#     xent_loss = original_dim * metrics.binary_crossentropy(x, x_decoded_mean)\n",
    "    xent_loss = original_dim * metrics.mean_squared_error(x, x_decoded_mean)\n",
    "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    return K.mean(xent_loss + kl_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = Model(x, x_decoded_mean)\n",
    "vae.compile(optimizer='adam', loss=vae_loss) #metrics=[xent_loss, kl_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# vae.predict(vectorized_train, batch_size=batch_size) - vectorized_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 125000 samples, validate on 53500 samples\n",
      "Epoch 1/200\n",
      "125000/125000 [==============================] - 28s 224us/step - loss: 8705.8093 - val_loss: 7783.2301\n",
      "\n",
      "Epoch 00001: saving model to /tmp/model.h5\n",
      "Epoch 2/200\n",
      "125000/125000 [==============================] - 19s 150us/step - loss: 7524.8679 - val_loss: 7353.1838\n",
      "\n",
      "Epoch 00002: saving model to /tmp/model.h5\n",
      "Epoch 3/200\n",
      "125000/125000 [==============================] - 19s 149us/step - loss: 7211.2379 - val_loss: 7131.5158\n",
      "\n",
      "Epoch 00003: saving model to /tmp/model.h5\n",
      "Epoch 4/200\n",
      "125000/125000 [==============================] - 19s 148us/step - loss: 7015.4422 - val_loss: 6980.6946\n",
      "\n",
      "Epoch 00004: saving model to /tmp/model.h5\n",
      "Epoch 5/200\n",
      "125000/125000 [==============================] - 19s 149us/step - loss: 6874.3819 - val_loss: 6865.3088\n",
      "\n",
      "Epoch 00005: saving model to /tmp/model.h5\n",
      "Epoch 6/200\n",
      "125000/125000 [==============================] - 19s 153us/step - loss: 6767.9304 - val_loss: 6780.2317\n",
      "\n",
      "Epoch 00006: saving model to /tmp/model.h5\n",
      "Epoch 7/200\n",
      "125000/125000 [==============================] - 19s 151us/step - loss: 6681.3884 - val_loss: 6715.5539\n",
      "\n",
      "Epoch 00007: saving model to /tmp/model.h5\n",
      "Epoch 8/200\n",
      "125000/125000 [==============================] - 18s 148us/step - loss: 6610.8453 - val_loss: 6652.6437\n",
      "\n",
      "Epoch 00008: saving model to /tmp/model.h5\n",
      "Epoch 9/200\n",
      "125000/125000 [==============================] - 19s 152us/step - loss: 6547.5215 - val_loss: 6610.0889\n",
      "\n",
      "Epoch 00009: saving model to /tmp/model.h5\n",
      "Epoch 10/200\n",
      "125000/125000 [==============================] - 19s 150us/step - loss: 6496.9459 - val_loss: 6569.8592\n",
      "\n",
      "Epoch 00010: saving model to /tmp/model.h5\n",
      "Epoch 11/200\n",
      "125000/125000 [==============================] - 19s 149us/step - loss: 6448.2750 - val_loss: 6531.2842\n",
      "\n",
      "Epoch 00011: saving model to /tmp/model.h5\n",
      "Epoch 12/200\n",
      "125000/125000 [==============================] - 19s 149us/step - loss: 6408.8267 - val_loss: 6505.3929\n",
      "\n",
      "Epoch 00012: saving model to /tmp/model.h5\n",
      "Epoch 13/200\n",
      "125000/125000 [==============================] - 19s 150us/step - loss: 6370.1459 - val_loss: 6475.1900\n",
      "\n",
      "Epoch 00013: saving model to /tmp/model.h5\n",
      "Epoch 14/200\n",
      "125000/125000 [==============================] - 19s 151us/step - loss: 6338.0810 - val_loss: 6455.1362\n",
      "\n",
      "Epoch 00014: saving model to /tmp/model.h5\n",
      "Epoch 15/200\n",
      "125000/125000 [==============================] - 19s 150us/step - loss: 6310.1732 - val_loss: 6430.3606\n",
      "\n",
      "Epoch 00015: saving model to /tmp/model.h5\n",
      "Epoch 16/200\n",
      "125000/125000 [==============================] - 19s 149us/step - loss: 6281.8375 - val_loss: 6414.9411\n",
      "\n",
      "Epoch 00016: saving model to /tmp/model.h5\n",
      "Epoch 17/200\n",
      "125000/125000 [==============================] - 18s 148us/step - loss: 6255.9918 - val_loss: 6391.1560\n",
      "\n",
      "Epoch 00017: saving model to /tmp/model.h5\n",
      "Epoch 18/200\n",
      "125000/125000 [==============================] - 19s 149us/step - loss: 6234.3637 - val_loss: 6378.8242\n",
      "\n",
      "Epoch 00018: saving model to /tmp/model.h5\n",
      "Epoch 19/200\n",
      "125000/125000 [==============================] - 19s 152us/step - loss: 6211.9527 - val_loss: 6360.7911\n",
      "\n",
      "Epoch 00019: saving model to /tmp/model.h5\n",
      "Epoch 20/200\n",
      "125000/125000 [==============================] - 19s 149us/step - loss: 6192.2673 - val_loss: 6349.2058\n",
      "\n",
      "Epoch 00020: saving model to /tmp/model.h5\n",
      "Epoch 21/200\n",
      "125000/125000 [==============================] - 18s 146us/step - loss: 6173.8501 - val_loss: 6336.1106\n",
      "\n",
      "Epoch 00021: saving model to /tmp/model.h5\n",
      "Epoch 22/200\n",
      "125000/125000 [==============================] - 19s 150us/step - loss: 6155.1759 - val_loss: 6328.2667\n",
      "\n",
      "Epoch 00022: saving model to /tmp/model.h5\n",
      "Epoch 23/200\n",
      "125000/125000 [==============================] - 19s 149us/step - loss: 6140.2327 - val_loss: 6314.6389\n",
      "\n",
      "Epoch 00023: saving model to /tmp/model.h5\n",
      "Epoch 24/200\n",
      "125000/125000 [==============================] - 19s 150us/step - loss: 6122.6437 - val_loss: 6306.2680\n",
      "\n",
      "Epoch 00024: saving model to /tmp/model.h5\n",
      "Epoch 25/200\n",
      "125000/125000 [==============================] - 19s 149us/step - loss: 6107.1461 - val_loss: 6289.9304\n",
      "\n",
      "Epoch 00025: saving model to /tmp/model.h5\n",
      "Epoch 26/200\n",
      " 90000/125000 [====================>.........] - ETA: 4s - loss: 6095.9532"
     ]
    }
   ],
   "source": [
    "#### checkpoint\n",
    "cp = [callbacks.ModelCheckpoint(filepath=\"/tmp/model.h5\", verbose=1)]\n",
    "\n",
    "#train\n",
    "vae.fit(vectorized_train, help_vectorized_train,\n",
    "        shuffle=True,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(vectorized_test, help_vectorized_test),\n",
    "        callbacks=cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a model to project inputs on the latent space\n",
    "encoder = Model(x, z_mean)\n",
    "\n",
    "# build a generator that can sample from the learned distribution\n",
    "decoder_input = Input(shape=( latent_dim,))\n",
    "_h_decoded = decoder_h(decoder_input)\n",
    "_x_decoded_mean = decoder_mean(_h_decoded)\n",
    "generator = Model(decoder_input, _x_decoded_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.4065683, 3.185115)"
      ]
     },
     "execution_count": 622,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model = Model(x, z_mean)\n",
    "np.min(test_model.predict(vectorized_test[0:10])), np.max(test_model.predict(vectorized_test[0:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Text From Latent Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some matrix magic\n",
    "def sent_parse(sentence, mat_shape):\n",
    "    data_concat = []\n",
    "    word_vecs = vectorize_sentences(sentence)\n",
    "    for x in word_vecs:\n",
    "        data_concat.append(list(itertools.chain.from_iterable(x)))\n",
    "    zero_matr = np.zeros(mat_shape)\n",
    "    zero_matr[0] = np.array(data_concat)\n",
    "    return zero_matr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sentence_with_w2v(sent_vect):\n",
    "#     sent_vect = sent_vect * (vectorized_padded.max() - vectorized_padded.min()) + vectorized_padded.min()\n",
    "    word_sent = ''\n",
    "    tocut = sent_vect\n",
    "    for i in range (int(len(sent_vect)/100)):\n",
    "        word_sent += w2v.most_similar(positive=[tocut[:100]], topn=1)[0][0]\n",
    "        word_sent += ' '\n",
    "        tocut = tocut[100:]\n",
    "    print(word_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: encoded sentence vector\n",
    "# output: encoded sentence vector in dataset with highest cosine similarity\n",
    "def find_similar_encoding(sent_vect):\n",
    "    all_cosine = []\n",
    "    for sent in sent_encoded:\n",
    "        result = 1 - spatial.distance.cosine(sent_vect, sent)\n",
    "        all_cosine.append(result)\n",
    "    data_array = np.array(all_cosine)\n",
    "    maximum = data_array.argsort()[-3:][::-1][1]\n",
    "    new_vec = sent_encoded[maximum]\n",
    "    return new_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: two points, integer n\n",
    "# output: n equidistant points on the line between the input points (inclusive)\n",
    "def shortest_homology(point_one, point_two, num):\n",
    "    dist_vec = point_two - point_one\n",
    "    sample = np.linspace(0, 1, num, endpoint = True)\n",
    "    hom_sample = []\n",
    "    for s in sample:\n",
    "        hom_sample.append(point_one + s * dist_vec)\n",
    "    return hom_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: two written sentences, VAE batch-size, dimension of VAE input\n",
    "# output: the function embeds the sentences in latent-space, and then prints their generated text representations\n",
    "# along with the text representations of several points in between them\n",
    "def sent_2_sent(sent1,sent2, batch, dim):\n",
    "    a = sent_parse([sent1], (batch,dim))\n",
    "    b = sent_parse([sent2], (batch,dim))\n",
    "    encode_a = encoder.predict(a, batch_size = batch)\n",
    "    encode_b = encoder.predict(b, batch_size = batch)\n",
    "    test_hom = hom_shortest(encode_a[0], encode_b[0], 5)\n",
    "    \n",
    "    for point in test_hom:\n",
    "        p = generator.predict(np.array([point]))[0]\n",
    "        print_sentence(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing sentences from the training set and comparing them with the original will test whether the custom print function works properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences = [vectorize_sentence(preprocess_text(\"zero should be less than or equal to zero\"))]\n",
    "# padded_sentences = sequence.pad_sequences(sentences, maxlen=original_dim, padding=\"post\", truncating=\"post\")\n",
    "padded_sentences = vectorized_train[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder takes the training set of sentence vectors (concatenanted word vectors) and embeds them into a lower dimensional vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_encoded = encoder.predict(padded_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 100)"
      ]
     },
     "execution_count": 630,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_encoded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder takes the list of latent dimensional encodings from above and turns them back into vectors of their original dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_decoded = generator.predict(sent_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11,\n",
       " 4,\n",
       " 4,\n",
       " -1,\n",
       " -7,\n",
       " -2,\n",
       " -3,\n",
       " -1,\n",
       " 4,\n",
       " 5,\n",
       " 0,\n",
       " -6,\n",
       " 1,\n",
       " -1,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 9,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " -3,\n",
       " 1,\n",
       " -5,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 1,\n",
       " 5,\n",
       " 0,\n",
       " -3,\n",
       " 0,\n",
       " -1,\n",
       " -2,\n",
       " 5,\n",
       " 0,\n",
       " -5,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -5,\n",
       " -7,\n",
       " -3,\n",
       " 7,\n",
       " 3,\n",
       " -3,\n",
       " 6,\n",
       " -4,\n",
       " 1,\n",
       " 5,\n",
       " -1,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 2,\n",
       " -4,\n",
       " 0,\n",
       " -3,\n",
       " -4,\n",
       " -7,\n",
       " 0,\n",
       " -5,\n",
       " -1,\n",
       " 6,\n",
       " -5,\n",
       " -1,\n",
       " 0,\n",
       " 3,\n",
       " 7,\n",
       " 5,\n",
       " -2,\n",
       " 5,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " -3,\n",
       " -4,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " -2,\n",
       " 0,\n",
       " -3,\n",
       " 0,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 0,\n",
       " -10,\n",
       " 4,\n",
       " -1,\n",
       " 0,\n",
       " 1,\n",
       " -3,\n",
       " -1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " -2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " -3,\n",
       " -3,\n",
       " -1,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " -2,\n",
       " 0,\n",
       " 1,\n",
       " -2,\n",
       " -1,\n",
       " 0,\n",
       " 0,\n",
       " -1,\n",
       " 2,\n",
       " 4,\n",
       " -4,\n",
       " 0,\n",
       " -1,\n",
       " 0,\n",
       " -1,\n",
       " -1,\n",
       " 0,\n",
       " 1,\n",
       " -1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " -2,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 0,\n",
       " -2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " -2,\n",
       " -1,\n",
       " 1,\n",
       " 3,\n",
       " -1,\n",
       " 1,\n",
       " -2,\n",
       " -2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " -2,\n",
       " -3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -2,\n",
       " 0,\n",
       " -4,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " -1,\n",
       " 0,\n",
       " -1,\n",
       " 0,\n",
       " -1,\n",
       " 0,\n",
       " 0,\n",
       " -1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " -2,\n",
       " 0,\n",
       " 0,\n",
       " -2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " -1,\n",
       " -2,\n",
       " 0,\n",
       " 1,\n",
       " -2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 2,\n",
       " -4,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " -1,\n",
       " -4,\n",
       " -1,\n",
       " 3,\n",
       " -1,\n",
       " -1,\n",
       " 2,\n",
       " -1,\n",
       " -4,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " -1,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " -1,\n",
       " -3,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " -1,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " -2,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " -4,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " -1,\n",
       " 1,\n",
       " -3,\n",
       " -1,\n",
       " 1,\n",
       " -2,\n",
       " -1,\n",
       " 2,\n",
       " -2,\n",
       " -2,\n",
       " 1,\n",
       " -1,\n",
       " -2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " -5,\n",
       " -5,\n",
       " -1,\n",
       " 0,\n",
       " -1,\n",
       " -5,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " -1,\n",
       " -1,\n",
       " -2,\n",
       " 0,\n",
       " -6,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " -4,\n",
       " 1,\n",
       " 0,\n",
       " -3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " -4,\n",
       " 0,\n",
       " 2,\n",
       " -2,\n",
       " -3,\n",
       " 0,\n",
       " -5,\n",
       " -4,\n",
       " 2,\n",
       " -5,\n",
       " 0,\n",
       " -5,\n",
       " -3,\n",
       " 0,\n",
       " -7,\n",
       " 3,\n",
       " -5,\n",
       " 4,\n",
       " 0,\n",
       " 8,\n",
       " 7,\n",
       " -1,\n",
       " -6,\n",
       " -2,\n",
       " 0,\n",
       " -1,\n",
       " 5,\n",
       " 2,\n",
       " 1,\n",
       " -1,\n",
       " 0,\n",
       " -4,\n",
       " -3,\n",
       " 0,\n",
       " 0,\n",
       " -1,\n",
       " -4,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " -4,\n",
       " 0,\n",
       " -2,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " -7,\n",
       " -2,\n",
       " -1,\n",
       " -5,\n",
       " 1,\n",
       " -1,\n",
       " 4,\n",
       " -1,\n",
       " -1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " -1,\n",
       " 3,\n",
       " -4,\n",
       " 0,\n",
       " 0,\n",
       " -4,\n",
       " 4,\n",
       " -1,\n",
       " -1,\n",
       " -6,\n",
       " 0,\n",
       " -5,\n",
       " 1,\n",
       " 3,\n",
       " -3,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 5,\n",
       " 3,\n",
       " 0,\n",
       " -1,\n",
       " -2,\n",
       " -1,\n",
       " 0,\n",
       " -2,\n",
       " -8,\n",
       " 4,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " -2,\n",
       " 6,\n",
       " -2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " -6,\n",
       " -5,\n",
       " -5,\n",
       " 0,\n",
       " 5,\n",
       " -2,\n",
       " -2,\n",
       " -2,\n",
       " 2,\n",
       " 2,\n",
       " -1,\n",
       " 2,\n",
       " 0,\n",
       " -7,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " -1,\n",
       " 1,\n",
       " 7,\n",
       " 0,\n",
       " -1,\n",
       " 5,\n",
       " 2,\n",
       " 1,\n",
       " -2,\n",
       " -2,\n",
       " 0,\n",
       " 4,\n",
       " -3,\n",
       " -6,\n",
       " 1,\n",
       " 3,\n",
       " 6,\n",
       " -1,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " -3,\n",
       " -3,\n",
       " -1,\n",
       " -2,\n",
       " 0,\n",
       " 4,\n",
       " -2,\n",
       " -2,\n",
       " -1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " -4,\n",
       " 5,\n",
       " -1,\n",
       " 1,\n",
       " 5,\n",
       " 0,\n",
       " -3,\n",
       " -2,\n",
       " -4,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " -4,\n",
       " 0,\n",
       " 0,\n",
       " -1,\n",
       " 0,\n",
       " -3,\n",
       " 0,\n",
       " -1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " -1,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " -1,\n",
       " -4,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " -5,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " -2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " -5,\n",
       " 6,\n",
       " 3,\n",
       " 0,\n",
       " -4,\n",
       " 0,\n",
       " 3,\n",
       " -3,\n",
       " 1,\n",
       " 0,\n",
       " -4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " -2,\n",
       " 2,\n",
       " -2,\n",
       " -1,\n",
       " -2,\n",
       " 8,\n",
       " 0,\n",
       " -4,\n",
       " 5,\n",
       " 0,\n",
       " -3,\n",
       " 1,\n",
       " -1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " -3,\n",
       " 3,\n",
       " 2,\n",
       " -1,\n",
       " -4,\n",
       " 0,\n",
       " -2,\n",
       " -2,\n",
       " -3,\n",
       " -1,\n",
       " -3,\n",
       " 0,\n",
       " 4,\n",
       " -1,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " -2,\n",
       " 2,\n",
       " -1,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " -4,\n",
       " 2,\n",
       " -2,\n",
       " -1,\n",
       " 2,\n",
       " 5,\n",
       " -5,\n",
       " -2,\n",
       " -3,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " -2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " -4,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " -2,\n",
       " -1,\n",
       " 4,\n",
       " 1,\n",
       " -4,\n",
       " 5,\n",
       " 1,\n",
       " -4,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " -3,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 2,\n",
       " 0,\n",
       " -5,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -5,\n",
       " -3,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " -1,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " -1,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " -2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " -3,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " -3,\n",
       " 1,\n",
       " 0,\n",
       " -1,\n",
       " -1,\n",
       " 2,\n",
       " -1,\n",
       " -3,\n",
       " 2,\n",
       " -3,\n",
       " 0,\n",
       " -1,\n",
       " 0,\n",
       " 0,\n",
       " -3,\n",
       " -4,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " -5,\n",
       " -1,\n",
       " 1,\n",
       " 3,\n",
       " -1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " -3,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " -4,\n",
       " -2,\n",
       " -3,\n",
       " -5,\n",
       " 4,\n",
       " 0,\n",
       " -1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " -5,\n",
       " 3,\n",
       " 0,\n",
       " -1,\n",
       " 2,\n",
       " 0,\n",
       " -7,\n",
       " 0,\n",
       " 3,\n",
       " -5,\n",
       " -2,\n",
       " -5,\n",
       " -2,\n",
       " 1,\n",
       " -1,\n",
       " 2,\n",
       " -8,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " -2,\n",
       " -8,\n",
       " -6,\n",
       " -1,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 6,\n",
       " 0,\n",
       " 5,\n",
       " -2,\n",
       " -2,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " -1,\n",
       " 4,\n",
       " -1,\n",
       " -2,\n",
       " 4,\n",
       " 0,\n",
       " -2,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " -2,\n",
       " -3,\n",
       " -1,\n",
       " 0,\n",
       " 0,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " -4,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 6,\n",
       " 0,\n",
       " -1,\n",
       " 2,\n",
       " -1,\n",
       " 8,\n",
       " -2,\n",
       " 0,\n",
       " -7,\n",
       " 1,\n",
       " -2,\n",
       " -1,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " -3,\n",
       " 0,\n",
       " 3,\n",
       " -3,\n",
       " 2,\n",
       " 0,\n",
       " 6,\n",
       " 5,\n",
       " 3,\n",
       " 0,\n",
       " -1,\n",
       " -2,\n",
       " -1,\n",
       " 0,\n",
       " -2,\n",
       " -8,\n",
       " 4,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " -2,\n",
       " 6,\n",
       " -2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " -6,\n",
       " -5,\n",
       " -5,\n",
       " 0,\n",
       " 5,\n",
       " -2,\n",
       " -2,\n",
       " -2,\n",
       " 2,\n",
       " 2,\n",
       " -1,\n",
       " 2,\n",
       " 0,\n",
       " -7,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " -1,\n",
       " 1,\n",
       " 7,\n",
       " 0,\n",
       " -1,\n",
       " 5,\n",
       " 2,\n",
       " 1,\n",
       " -2,\n",
       " -2,\n",
       " 0,\n",
       " 4,\n",
       " -3,\n",
       " -6,\n",
       " 1,\n",
       " 3,\n",
       " 6,\n",
       " -1,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " -3,\n",
       " -3,\n",
       " -1,\n",
       " -2,\n",
       " 0,\n",
       " 4,\n",
       " -2,\n",
       " -2,\n",
       " -1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " -4,\n",
       " 5,\n",
       " -1,\n",
       " 1,\n",
       " 5,\n",
       " 0,\n",
       " -3,\n",
       " -2,\n",
       " -4,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " -4,\n",
       " 0,\n",
       " 0,\n",
       " -1,\n",
       " 0,\n",
       " -3,\n",
       " 0,\n",
       " -1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " -1,\n",
       " -2,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -2,\n",
       " -3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " -1,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " -1,\n",
       " -2,\n",
       " -1,\n",
       " -1,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " -1,\n",
       " -2,\n",
       " 0,\n",
       " 0,\n",
       " -2,\n",
       " 0,\n",
       " -1,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " -5,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " -3,\n",
       " 0,\n",
       " -1,\n",
       " -1,\n",
       " -2,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " -2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " -2,\n",
       " -2,\n",
       " 0,\n",
       " -2,\n",
       " -1,\n",
       " 0,\n",
       " 0,\n",
       " -2,\n",
       " 0,\n",
       " 1,\n",
       " -1,\n",
       " -3,\n",
       " 0,\n",
       " 1,\n",
       " -2,\n",
       " 0,\n",
       " 2,\n",
       " -1,\n",
       " 3,\n",
       " ...]"
      ]
     },
     "execution_count": 632,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_train[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10.292098999023438,\n",
       " 4.007075309753418,\n",
       " 3.8734469413757324,\n",
       " -0.3958585560321808,\n",
       " -5.327584266662598,\n",
       " -1.147791862487793,\n",
       " -2.847898244857788,\n",
       " -0.7675120830535889,\n",
       " 2.911266326904297,\n",
       " 4.017035484313965,\n",
       " -0.3958999514579773,\n",
       " -6.533461093902588,\n",
       " 1.2897509336471558,\n",
       " -0.4881402254104614,\n",
       " 1.6625556945800781,\n",
       " 2.7773900032043457,\n",
       " -0.22869598865509033,\n",
       " 1.9939302206039429,\n",
       " 7.99796199798584,\n",
       " 4.870890140533447,\n",
       " 1.6457968950271606,\n",
       " 1.8600523471832275,\n",
       " -0.014736175537109375,\n",
       " -2.4909653663635254,\n",
       " 0.18179559707641602,\n",
       " -4.635962009429932,\n",
       " 1.6137866973876953,\n",
       " 6.973522663116455,\n",
       " 7.398083686828613,\n",
       " 1.1160871982574463,\n",
       " 4.805074214935303,\n",
       " 0.11988791823387146,\n",
       " -2.885666608810425,\n",
       " 0.5359637141227722,\n",
       " -0.8296325206756592,\n",
       " -1.6863164901733398,\n",
       " 3.921964645385742,\n",
       " -0.824578046798706,\n",
       " -5.026695251464844,\n",
       " 0.87347012758255,\n",
       " 0.7698076367378235,\n",
       " -0.47742265462875366,\n",
       " -0.3201391100883484,\n",
       " -4.602727890014648,\n",
       " -5.579717636108398,\n",
       " -2.6802244186401367,\n",
       " 6.1884355545043945,\n",
       " 2.413776159286499,\n",
       " -3.036322832107544,\n",
       " 5.345203399658203,\n",
       " -3.9442481994628906,\n",
       " 1.293487548828125,\n",
       " 4.992898941040039,\n",
       " -1.0071990489959717,\n",
       " 3.098315954208374,\n",
       " 0.7694389224052429,\n",
       " 0.08024021983146667,\n",
       " -0.5467199087142944,\n",
       " 3.9870009422302246,\n",
       " -0.4625025689601898,\n",
       " 1.8081235885620117,\n",
       " -3.1272881031036377,\n",
       " -0.17313480377197266,\n",
       " -3.426978826522827,\n",
       " -3.566441535949707,\n",
       " -7.0884623527526855,\n",
       " 0.15241119265556335,\n",
       " -4.503772735595703,\n",
       " -0.6130039691925049,\n",
       " 4.8924407958984375,\n",
       " -4.73352575302124,\n",
       " -0.7092550992965698,\n",
       " -0.020257554948329926,\n",
       " 2.97645902633667,\n",
       " 5.704624652862549,\n",
       " 4.593249320983887,\n",
       " -1.7723453044891357,\n",
       " 4.527212142944336,\n",
       " 2.2578532695770264,\n",
       " -0.33411651849746704,\n",
       " 0.3766760230064392,\n",
       " 0.6768606901168823,\n",
       " 0.02369821071624756,\n",
       " 5.2083234786987305,\n",
       " -2.9395065307617188,\n",
       " -3.5348641872406006,\n",
       " 4.153532981872559,\n",
       " 2.4033570289611816,\n",
       " 1.5910770893096924,\n",
       " 2.079711437225342,\n",
       " -1.8371171951293945,\n",
       " -0.38552162051200867,\n",
       " -2.5506482124328613,\n",
       " -0.23413392901420593,\n",
       " 1.3313037157058716,\n",
       " -0.1414845585823059,\n",
       " -1.0497791767120361,\n",
       " -0.13504353165626526,\n",
       " -9.69404411315918,\n",
       " 3.776970624923706,\n",
       " -1.6994858980178833,\n",
       " 0.24787954986095428,\n",
       " 0.8085159659385681,\n",
       " -0.667953372001648,\n",
       " -0.5726476907730103,\n",
       " -0.2521835267543793,\n",
       " -2.1999993324279785,\n",
       " -0.05033528804779053,\n",
       " 0.7339505553245544,\n",
       " -0.08208223432302475,\n",
       " 0.6320277452468872,\n",
       " 0.24972198903560638,\n",
       " 0.9608099460601807,\n",
       " 0.5059711337089539,\n",
       " 0.4444844722747803,\n",
       " -1.7638771533966064,\n",
       " -1.2849692106246948,\n",
       " -0.18977150321006775,\n",
       " -0.7744471430778503,\n",
       " -1.5741320848464966,\n",
       " -0.9159201979637146,\n",
       " 0.09649408608675003,\n",
       " -1.0997114181518555,\n",
       " 0.014349102973937988,\n",
       " 0.9199314117431641,\n",
       " -0.11482634395360947,\n",
       " 0.4089300036430359,\n",
       " -1.8924883604049683,\n",
       " 1.1332052946090698,\n",
       " -0.4348588287830353,\n",
       " 1.022457480430603,\n",
       " -0.7550877332687378,\n",
       " 1.3406966924667358,\n",
       " -0.08380991220474243,\n",
       " 0.5794162154197693,\n",
       " 2.1628875732421875,\n",
       " -1.2098345756530762,\n",
       " -1.4477540254592896,\n",
       " 0.6276822090148926,\n",
       " 0.40897423028945923,\n",
       " 2.225031614303589,\n",
       " 0.6121461391448975,\n",
       " 0.7175837755203247,\n",
       " 0.41933852434158325,\n",
       " -0.9746900796890259,\n",
       " -0.07939401268959045,\n",
       " -0.16529779136180878,\n",
       " 0.8263382315635681,\n",
       " -1.2658042907714844,\n",
       " -0.9207127094268799,\n",
       " 0.0536092072725296,\n",
       " 0.6240925192832947,\n",
       " -0.1419409215450287,\n",
       " 1.5387954711914062,\n",
       " -0.9660745859146118,\n",
       " -1.1535391807556152,\n",
       " -1.185330867767334,\n",
       " 1.3567638397216797,\n",
       " 0.25355881452560425,\n",
       " -0.569282054901123,\n",
       " -2.2469935417175293,\n",
       " -0.03440527617931366,\n",
       " 0.22130456566810608,\n",
       " -1.5661146640777588,\n",
       " -0.5139332413673401,\n",
       " -0.5589396357536316,\n",
       " 0.2507037818431854,\n",
       " -0.7236634492874146,\n",
       " 1.0901167392730713,\n",
       " 1.649221420288086,\n",
       " -0.7569531798362732,\n",
       " 0.175511434674263,\n",
       " 2.5119903087615967,\n",
       " -1.3849284648895264,\n",
       " 0.17968016862869263,\n",
       " -0.8464058637619019,\n",
       " -1.0818967819213867,\n",
       " 0.7261011004447937,\n",
       " 0.7499066591262817,\n",
       " -1.0071276426315308,\n",
       " -0.6293188333511353,\n",
       " -0.07520026713609695,\n",
       " 0.10774359107017517,\n",
       " 0.37892019748687744,\n",
       " 1.067836880683899,\n",
       " 0.5596761107444763,\n",
       " 0.6017728447914124,\n",
       " -2.1967368125915527,\n",
       " -0.23758795857429504,\n",
       " 0.6942472457885742,\n",
       " -1.4272558689117432,\n",
       " -0.607852578163147,\n",
       " 2.2107362747192383,\n",
       " -1.3363615274429321,\n",
       " -0.4755728244781494,\n",
       " -1.1737496852874756,\n",
       " 1.8422996997833252,\n",
       " -0.7514457702636719,\n",
       " 1.191766619682312,\n",
       " 1.6139566898345947,\n",
       " 1.0602420568466187,\n",
       " 0.049139805138111115,\n",
       " -0.3036333918571472,\n",
       " 0.6415977478027344,\n",
       " 0.30039143562316895,\n",
       " -0.789164662361145,\n",
       " 0.016886427998542786,\n",
       " 0.6059186458587646,\n",
       " -0.6485346555709839,\n",
       " -0.890635073184967,\n",
       " -0.3160100281238556,\n",
       " 1.7295730113983154,\n",
       " -0.38554784655570984,\n",
       " -0.335907518863678,\n",
       " 0.1827557384967804,\n",
       " -0.1879747211933136,\n",
       " 0.43157175183296204,\n",
       " 0.8502938747406006,\n",
       " 0.9594548344612122,\n",
       " -1.486382246017456,\n",
       " 1.6970146894454956,\n",
       " -0.7084632515907288,\n",
       " 0.13851165771484375,\n",
       " -1.1917392015457153,\n",
       " 0.8705432415008545,\n",
       " -1.5677270889282227,\n",
       " -0.3555937707424164,\n",
       " -0.7099798917770386,\n",
       " 0.010570250451564789,\n",
       " -0.24906137585639954,\n",
       " 0.48017212748527527,\n",
       " -1.8554993867874146,\n",
       " 1.312366008758545,\n",
       " 2.0158727169036865,\n",
       " 0.8626701831817627,\n",
       " 0.20146730542182922,\n",
       " 1.0685402154922485,\n",
       " 0.5295150279998779,\n",
       " -1.835620641708374,\n",
       " -0.3534910976886749,\n",
       " 0.11727568507194519,\n",
       " -0.43820035457611084,\n",
       " 0.5911356210708618,\n",
       " 0.6871781349182129,\n",
       " 0.802365243434906,\n",
       " 0.016099512577056885,\n",
       " 0.2791861891746521,\n",
       " 1.1551313400268555,\n",
       " 0.08361528068780899,\n",
       " 0.13647890090942383,\n",
       " -0.8588744401931763,\n",
       " 0.053090035915374756,\n",
       " -0.12794806063175201,\n",
       " -1.0137907266616821,\n",
       " -0.49718207120895386,\n",
       " -0.6933412551879883,\n",
       " 1.1583462953567505,\n",
       " -0.45822712779045105,\n",
       " 0.21020720899105072,\n",
       " 0.5507078170776367,\n",
       " -0.0868573859333992,\n",
       " 0.23329739272594452,\n",
       " 0.40106484293937683,\n",
       " 0.13397987186908722,\n",
       " 1.040761947631836,\n",
       " 1.3345621824264526,\n",
       " -0.06712108105421066,\n",
       " 0.37929677963256836,\n",
       " 0.035826098173856735,\n",
       " 0.004453040659427643,\n",
       " 0.04479740560054779,\n",
       " 0.0231640487909317,\n",
       " -0.46388301253318787,\n",
       " -0.5486512184143066,\n",
       " 0.38104432821273804,\n",
       " -0.1580725759267807,\n",
       " -0.11631108820438385,\n",
       " -0.7614466547966003,\n",
       " -0.16385535895824432,\n",
       " -0.7583919763565063,\n",
       " -0.17695680260658264,\n",
       " 0.35335803031921387,\n",
       " -0.6732257008552551,\n",
       " 0.12789562344551086,\n",
       " -0.5529019832611084,\n",
       " -0.6032013893127441,\n",
       " 1.0294476747512817,\n",
       " 0.7341355085372925,\n",
       " 0.3639727830886841,\n",
       " -0.027474619448184967,\n",
       " -0.7594414353370667,\n",
       " -1.2678040266036987,\n",
       " 0.4990893304347992,\n",
       " -0.23778167366981506,\n",
       " -0.983841061592102,\n",
       " -0.07774931937456131,\n",
       " -0.535183846950531,\n",
       " 1.1491564512252808,\n",
       " -0.043556295335292816,\n",
       " -0.35093212127685547,\n",
       " 2.0454204082489014,\n",
       " 0.7631636261940002,\n",
       " -3.507178544998169,\n",
       " 2.259260654449463,\n",
       " 2.7951414585113525,\n",
       " 2.058342218399048,\n",
       " 0.0975189208984375,\n",
       " -2.7669336795806885,\n",
       " -1.960997223854065,\n",
       " -0.47202110290527344,\n",
       " 3.0304319858551025,\n",
       " -1.4846240282058716,\n",
       " -0.7710026502609253,\n",
       " 0.6730464696884155,\n",
       " 1.5635762214660645,\n",
       " 0.8448910117149353,\n",
       " -1.4373332262039185,\n",
       " -1.2825672626495361,\n",
       " 0.24572530388832092,\n",
       " 0.32225584983825684,\n",
       " -0.3286488652229309,\n",
       " 0.1886729598045349,\n",
       " -4.80501127243042,\n",
       " -1.6068934202194214,\n",
       " 0.6231433153152466,\n",
       " -2.760920524597168,\n",
       " -0.8951056599617004,\n",
       " -1.3918825387954712,\n",
       " -0.8541756272315979,\n",
       " -0.5633509159088135,\n",
       " -0.5938547253608704,\n",
       " 1.4512221813201904,\n",
       " -4.302198886871338,\n",
       " 2.9730875492095947,\n",
       " 2.0821144580841064,\n",
       " 2.5127320289611816,\n",
       " 2.3913090229034424,\n",
       " -0.5761452913284302,\n",
       " -5.025273323059082,\n",
       " -0.5760616064071655,\n",
       " 0.738854706287384,\n",
       " -0.20386600494384766,\n",
       " 2.8048512935638428,\n",
       " 1.1006529331207275,\n",
       " 1.9461231231689453,\n",
       " -1.0426934957504272,\n",
       " 2.589383363723755,\n",
       " -3.3293042182922363,\n",
       " -1.231455683708191,\n",
       " -0.3565203845500946,\n",
       " 1.9282671213150024,\n",
       " -0.1952657252550125,\n",
       " -1.6934736967086792,\n",
       " 0.8640629053115845,\n",
       " -1.0106309652328491,\n",
       " -0.2788524031639099,\n",
       " 1.7475603818893433,\n",
       " -0.5968677401542664,\n",
       " 1.1870484352111816,\n",
       " 0.35307788848876953,\n",
       " 2.9844160079956055,\n",
       " 1.8137190341949463,\n",
       " 1.3507616519927979,\n",
       " -0.11886367946863174,\n",
       " -2.5014805793762207,\n",
       " -0.16448341310024261,\n",
       " 1.3588125705718994,\n",
       " -1.4029728174209595,\n",
       " 0.28276437520980835,\n",
       " -1.780905842781067,\n",
       " 1.6387845277786255,\n",
       " -0.28533509373664856,\n",
       " -0.5530155897140503,\n",
       " 0.20935910940170288,\n",
       " 0.34340032935142517,\n",
       " 1.038098692893982,\n",
       " -0.3159055709838867,\n",
       " 2.098032236099243,\n",
       " -1.3496861457824707,\n",
       " -1.6597213745117188,\n",
       " -1.0645499229431152,\n",
       " -0.5962197780609131,\n",
       " 3.1427581310272217,\n",
       " -0.025045163929462433,\n",
       " -0.838036298751831,\n",
       " -1.6433645486831665,\n",
       " 0.5322751402854919,\n",
       " -2.117002010345459,\n",
       " 0.7744247317314148,\n",
       " 1.0586427450180054,\n",
       " -0.47274160385131836,\n",
       " 0.5257911682128906,\n",
       " 2.0026299953460693,\n",
       " 0.5704206824302673,\n",
       " 0.4017972946166992,\n",
       " 0.23764784634113312,\n",
       " 0.33257004618644714,\n",
       " 0.4914127588272095,\n",
       " -0.3231714367866516,\n",
       " 0.04256332665681839,\n",
       " -0.6254768371582031,\n",
       " 0.6990995407104492,\n",
       " 1.2567660808563232,\n",
       " 0.9030723571777344,\n",
       " -0.6645089983940125,\n",
       " -0.17000535130500793,\n",
       " -0.7786866426467896,\n",
       " 1.3192877769470215,\n",
       " 1.5759637355804443,\n",
       " 0.19194325804710388,\n",
       " -0.9524794816970825,\n",
       " 0.8776509761810303,\n",
       " 1.5530644655227661,\n",
       " -1.8334152698516846,\n",
       " 0.8314529657363892,\n",
       " 2.8072099685668945,\n",
       " 0.38634198904037476,\n",
       " 1.1200388669967651,\n",
       " -0.36032024025917053,\n",
       " -1.769173502922058,\n",
       " 1.0518945455551147,\n",
       " 0.7783003449440002,\n",
       " 3.858151435852051,\n",
       " 1.9613876342773438,\n",
       " -0.14766797423362732,\n",
       " -1.4989265203475952,\n",
       " 0.2402070164680481,\n",
       " 1.6356914043426514,\n",
       " 0.03461693227291107,\n",
       " 1.4058822393417358,\n",
       " 0.6745737195014954,\n",
       " -0.2534857392311096,\n",
       " -0.3482216000556946,\n",
       " 1.8444485664367676,\n",
       " 0.45282936096191406,\n",
       " -1.2134171724319458,\n",
       " 1.1921981573104858,\n",
       " 1.2374565601348877,\n",
       " 0.14595694839954376,\n",
       " 0.32023197412490845,\n",
       " -2.079768657684326,\n",
       " 0.6457676887512207,\n",
       " -1.6943809986114502,\n",
       " -0.08540903776884079,\n",
       " -0.06008772552013397,\n",
       " -0.4380503296852112,\n",
       " -0.4464014768600464,\n",
       " 2.0552730560302734,\n",
       " 1.7491827011108398,\n",
       " 0.9501448273658752,\n",
       " -1.0098066329956055,\n",
       " 1.0178265571594238,\n",
       " 0.2161569893360138,\n",
       " -0.25734323263168335,\n",
       " -0.1208854615688324,\n",
       " 0.0575307160615921,\n",
       " 1.0806692838668823,\n",
       " -0.8780836462974548,\n",
       " 1.8667314052581787,\n",
       " 0.9295810461044312,\n",
       " -0.2949426770210266,\n",
       " -0.6377323269844055,\n",
       " -0.10889457166194916,\n",
       " -0.8196471333503723,\n",
       " 0.28563398122787476,\n",
       " -0.7074882984161377,\n",
       " -0.09776613116264343,\n",
       " 1.313955307006836,\n",
       " -0.05883386731147766,\n",
       " -1.693066120147705,\n",
       " -0.023161880671977997,\n",
       " -0.3399016261100769,\n",
       " -0.4285021126270294,\n",
       " -1.332276463508606,\n",
       " 0.1467721164226532,\n",
       " -2.855975389480591,\n",
       " -0.004812959581613541,\n",
       " -0.9553310871124268,\n",
       " 1.979878544807434,\n",
       " 2.034822463989258,\n",
       " -1.5043514966964722,\n",
       " -0.6015774011611938,\n",
       " -0.8258355259895325,\n",
       " -0.38640910387039185,\n",
       " -0.6905403137207031,\n",
       " 0.6490998268127441,\n",
       " 2.9337687492370605,\n",
       " -1.1688119173049927,\n",
       " -0.7790461778640747,\n",
       " -0.9912251234054565,\n",
       " 0.8364040851593018,\n",
       " -0.6652805209159851,\n",
       " -1.4523142576217651,\n",
       " -0.7038065195083618,\n",
       " -0.6499481797218323,\n",
       " -0.22523924708366394,\n",
       " -0.09476309269666672,\n",
       " 0.025373023003339767,\n",
       " 0.03176786005496979,\n",
       " 0.7743025422096252,\n",
       " 4.772607326507568,\n",
       " -0.05638387054204941,\n",
       " 1.242417573928833,\n",
       " 2.035799026489258,\n",
       " 2.9347572326660156,\n",
       " -1.653712511062622,\n",
       " -2.4752390384674072,\n",
       " -0.0867239311337471,\n",
       " 4.45032262802124,\n",
       " 1.061282753944397,\n",
       " -1.0337103605270386,\n",
       " -4.630034446716309,\n",
       " 1.60152006149292,\n",
       " 3.730017900466919,\n",
       " 6.426272392272949,\n",
       " 0.7417648434638977,\n",
       " 0.15752896666526794,\n",
       " -0.49210983514785767,\n",
       " 5.627227306365967,\n",
       " -1.481456995010376,\n",
       " 1.51239812374115,\n",
       " 0.8148642778396606,\n",
       " 1.0107194185256958,\n",
       " 2.642711639404297,\n",
       " 0.832970917224884,\n",
       " -4.38935661315918,\n",
       " 4.719094276428223,\n",
       " 1.898797631263733,\n",
       " -1.3965307474136353,\n",
       " -3.0689284801483154,\n",
       " 0.5612373948097229,\n",
       " 1.994519591331482,\n",
       " -0.9650636911392212,\n",
       " 0.9789026379585266,\n",
       " -0.522031307220459,\n",
       " -2.762554883956909,\n",
       " -0.7056595683097839,\n",
       " -0.1691923588514328,\n",
       " 1.1244868040084839,\n",
       " -0.00758705660700798,\n",
       " 1.1587591171264648,\n",
       " -1.4602822065353394,\n",
       " 0.6613319516181946,\n",
       " -1.8789016008377075,\n",
       " -1.3101921081542969,\n",
       " -0.5692780613899231,\n",
       " 5.540747165679932,\n",
       " -0.030339833348989487,\n",
       " -2.1636664867401123,\n",
       " 3.264965772628784,\n",
       " 0.5745311975479126,\n",
       " -2.751845121383667,\n",
       " 0.5154780149459839,\n",
       " -0.5393228530883789,\n",
       " -0.4630450904369354,\n",
       " -0.014929968863725662,\n",
       " 1.6943556070327759,\n",
       " -0.6502005457878113,\n",
       " 2.689014434814453,\n",
       " 0.6669438481330872,\n",
       " -0.48690757155418396,\n",
       " -3.0752694606781006,\n",
       " -1.0302832126617432,\n",
       " -3.034018039703369,\n",
       " -1.3601946830749512,\n",
       " -2.21966552734375,\n",
       " 0.06405477225780487,\n",
       " -1.8878666162490845,\n",
       " -0.8839397430419922,\n",
       " 4.5620880126953125,\n",
       " 0.046758342534303665,\n",
       " 1.2151694297790527,\n",
       " 2.5343587398529053,\n",
       " -0.20212802290916443,\n",
       " 1.3599709272384644,\n",
       " 1.4808546304702759,\n",
       " -0.17424853146076202,\n",
       " 0.6387761831283569,\n",
       " -0.9615407586097717,\n",
       " 1.3244657516479492,\n",
       " -0.09543394297361374,\n",
       " 0.24914464354515076,\n",
       " -3.3358073234558105,\n",
       " 1.3665872812271118,\n",
       " -1.4959988594055176,\n",
       " -0.31186771392822266,\n",
       " 0.7655812501907349,\n",
       " 4.386542797088623,\n",
       " -5.3248796463012695,\n",
       " -1.8619118928909302,\n",
       " -2.8942887783050537,\n",
       " 1.2505921125411987,\n",
       " 2.414445638656616,\n",
       " -0.8733845949172974,\n",
       " 0.590186595916748,\n",
       " -0.4921661615371704,\n",
       " -1.5482847690582275,\n",
       " -0.03009115904569626,\n",
       " 2.4133787155151367,\n",
       " 0.7636532187461853,\n",
       " 1.1289819478988647,\n",
       " 1.5679945945739746,\n",
       " 1.706660509109497,\n",
       " 0.04713316261768341,\n",
       " -2.3857474327087402,\n",
       " 0.632570743560791,\n",
       " -0.19358819723129272,\n",
       " 1.356022834777832,\n",
       " 1.7773948907852173,\n",
       " 0.7179946303367615,\n",
       " -0.7095158100128174,\n",
       " -0.4118666648864746,\n",
       " -0.7722647786140442,\n",
       " 0.6705036163330078,\n",
       " -1.0623900890350342,\n",
       " 0.17409385740756989,\n",
       " -0.4627390503883362,\n",
       " 0.9187850952148438,\n",
       " 0.46556776762008667,\n",
       " -0.8242523670196533,\n",
       " -2.1252024173736572,\n",
       " 0.8790947794914246,\n",
       " 3.993560314178467,\n",
       " 1.5847468376159668,\n",
       " 1.02821683883667,\n",
       " -0.059471458196640015,\n",
       " -1.8251996040344238,\n",
       " -0.3423750400543213,\n",
       " 1.2463141679763794,\n",
       " 2.2681331634521484,\n",
       " 0.5837047100067139,\n",
       " -1.6267132759094238,\n",
       " 0.948943018913269,\n",
       " 0.7394435405731201,\n",
       " -0.8169267177581787,\n",
       " -0.6306844353675842,\n",
       " 0.7923470735549927,\n",
       " 0.987307071685791,\n",
       " 0.45911073684692383,\n",
       " 0.5556035041809082,\n",
       " -1.1389429569244385,\n",
       " 1.8892865180969238,\n",
       " 0.5262662172317505,\n",
       " 0.3375300467014313,\n",
       " -0.6909045577049255,\n",
       " 1.658474087715149,\n",
       " 1.1793179512023926,\n",
       " 1.6397007703781128,\n",
       " 1.6475619077682495,\n",
       " 1.7342228889465332,\n",
       " -0.5976547598838806,\n",
       " -0.03500092774629593,\n",
       " 1.0702766180038452,\n",
       " -0.39710694551467896,\n",
       " 0.9127623438835144,\n",
       " -1.3878122568130493,\n",
       " 0.6401274800300598,\n",
       " 1.0274436473846436,\n",
       " -0.3088918924331665,\n",
       " -1.2374464273452759,\n",
       " -0.9332714080810547,\n",
       " -1.177748680114746,\n",
       " -0.07708708941936493,\n",
       " 0.7789572477340698,\n",
       " -0.27224013209342957,\n",
       " -2.589527130126953,\n",
       " -0.22331400215625763,\n",
       " -1.3240282535552979,\n",
       " 0.4974825382232666,\n",
       " 0.35491126775741577,\n",
       " -0.7601640224456787,\n",
       " 0.17546899616718292,\n",
       " 1.0793853998184204,\n",
       " -1.3229553699493408,\n",
       " -1.0358163118362427,\n",
       " -2.001439332962036,\n",
       " -1.7003252506256104,\n",
       " 0.2346513569355011,\n",
       " -0.4888561964035034,\n",
       " -0.3125000596046448,\n",
       " 0.6152800917625427,\n",
       " -1.4991966485977173,\n",
       " -1.2315725088119507,\n",
       " 1.8527146577835083,\n",
       " 0.21081949770450592,\n",
       " -1.376315712928772,\n",
       " 1.37548828125,\n",
       " -1.2645373344421387,\n",
       " -0.18245825171470642,\n",
       " 0.34926724433898926,\n",
       " 0.8582619428634644,\n",
       " -2.3860230445861816,\n",
       " -0.4045787751674652,\n",
       " -0.7449547052383423,\n",
       " -1.8893409967422485,\n",
       " -0.7889865636825562,\n",
       " 0.7593765258789062,\n",
       " -0.16631770133972168,\n",
       " 0.9369170665740967,\n",
       " 2.665523052215576,\n",
       " -1.0614320039749146,\n",
       " 0.17527785897254944,\n",
       " -0.8411878943443298,\n",
       " 0.5113317966461182,\n",
       " 1.0794061422348022,\n",
       " -0.31803029775619507,\n",
       " -0.8366883397102356,\n",
       " 0.10350319743156433,\n",
       " -0.5560115575790405,\n",
       " -0.18546490371227264,\n",
       " 0.4959573447704315,\n",
       " -0.7924078702926636,\n",
       " -1.055378794670105,\n",
       " -0.9545738101005554,\n",
       " 1.605012059211731,\n",
       " -0.5833301544189453,\n",
       " 0.2769707441329956,\n",
       " 0.7427637577056885,\n",
       " -0.6523545384407043,\n",
       " -0.05788407474756241,\n",
       " 1.0665175914764404,\n",
       " -0.3534592092037201,\n",
       " -3.482389450073242,\n",
       " -0.9198978543281555,\n",
       " 0.4639192819595337,\n",
       " -0.5207419991493225,\n",
       " -1.712192416191101,\n",
       " -1.174996018409729,\n",
       " 0.7846445441246033,\n",
       " 0.7791993021965027,\n",
       " -1.0539048910140991,\n",
       " 0.6980288028717041,\n",
       " -1.8980737924575806,\n",
       " 1.183509349822998,\n",
       " 1.3484636545181274,\n",
       " 0.34471750259399414,\n",
       " 0.7216701507568359,\n",
       " -1.4272624254226685,\n",
       " -2.059528350830078,\n",
       " -0.45279234647750854,\n",
       " -0.07590410113334656,\n",
       " -0.19548392295837402,\n",
       " 0.052738338708877563,\n",
       " 0.6294159293174744,\n",
       " 0.4304431080818176,\n",
       " -1.6222572326660156,\n",
       " 2.7024827003479004,\n",
       " -1.331106185913086,\n",
       " -1.074385643005371,\n",
       " -1.8663469552993774,\n",
       " 1.02407968044281,\n",
       " 1.3724547624588013,\n",
       " -0.3749238848686218,\n",
       " 0.005468458868563175,\n",
       " -0.7328000664710999,\n",
       " 0.09530472010374069,\n",
       " 2.1022307872772217,\n",
       " -0.5900719165802002,\n",
       " 0.7747608423233032,\n",
       " 0.1748022437095642,\n",
       " -0.7285664081573486,\n",
       " 1.9775996208190918,\n",
       " -0.08286315202713013,\n",
       " -0.40713202953338623,\n",
       " -0.819525420665741,\n",
       " -0.9750508666038513,\n",
       " 0.07228904217481613,\n",
       " -1.008901834487915,\n",
       " -1.2910295724868774,\n",
       " -1.3779339790344238,\n",
       " -0.7965425848960876,\n",
       " 0.023644380271434784,\n",
       " 0.7603731751441956,\n",
       " -0.16295123100280762,\n",
       " 1.101117491722107,\n",
       " 0.7511931657791138,\n",
       " 1.7040774822235107,\n",
       " 1.9608820676803589,\n",
       " -1.0084500312805176,\n",
       " -1.5906493663787842,\n",
       " -0.0728568285703659,\n",
       " -0.8045811057090759,\n",
       " 0.6129053831100464,\n",
       " -0.6470222473144531,\n",
       " 0.7171745896339417,\n",
       " -0.8246287107467651,\n",
       " 1.2843040227890015,\n",
       " -0.5396316647529602,\n",
       " -1.2949360609054565,\n",
       " 1.603959083557129,\n",
       " -1.3319177627563477,\n",
       " 0.9967955350875854,\n",
       " -0.004036456346511841,\n",
       " 0.9479535818099976,\n",
       " -1.04033362865448,\n",
       " -0.2789003252983093,\n",
       " 0.8429930210113525,\n",
       " 0.515045702457428,\n",
       " -0.5026453137397766,\n",
       " 0.7928553223609924,\n",
       " 0.2526330351829529,\n",
       " 1.4211562871932983,\n",
       " 0.17755745351314545,\n",
       " -1.3033020496368408,\n",
       " -0.567709743976593,\n",
       " -0.5193648934364319,\n",
       " 0.07945942878723145,\n",
       " 0.14494948089122772,\n",
       " 0.818267822265625,\n",
       " -0.23724734783172607,\n",
       " -0.5754548907279968,\n",
       " -0.22554264962673187,\n",
       " 1.0777331590652466,\n",
       " 0.20536328852176666,\n",
       " 1.0438835620880127,\n",
       " 0.35272201895713806,\n",
       " 1.1308691501617432,\n",
       " -0.2631877660751343,\n",
       " 0.4242061674594879,\n",
       " -0.8720730543136597,\n",
       " 0.41907331347465515,\n",
       " 0.16097399592399597,\n",
       " 1.20669424533844,\n",
       " 0.34381017088890076,\n",
       " 1.1608787775039673,\n",
       " 0.03203459829092026,\n",
       " -0.7811011075973511,\n",
       " -0.35281893610954285,\n",
       " -0.003138132393360138,\n",
       " 0.35987991094589233,\n",
       " -0.8901643753051758,\n",
       " 0.5001069903373718,\n",
       " -0.03196294233202934,\n",
       " 0.6835833787918091,\n",
       " 0.8906043767929077,\n",
       " -0.23331789672374725,\n",
       " 0.4863486886024475,\n",
       " -0.5975977778434753,\n",
       " 0.4802650213241577,\n",
       " -0.06562353670597076,\n",
       " -1.3399237394332886,\n",
       " 1.2191590070724487,\n",
       " 0.708729088306427,\n",
       " 0.16214720904827118,\n",
       " 0.5642054677009583,\n",
       " -0.06152040511369705,\n",
       " -0.733343243598938,\n",
       " 1.567528247833252,\n",
       " 0.33348581194877625,\n",
       " 1.9841444492340088,\n",
       " -0.1387382447719574,\n",
       " -2.0961904525756836,\n",
       " -0.433816522359848,\n",
       " 0.45892569422721863,\n",
       " -0.7649496793746948,\n",
       " -0.8396064639091492,\n",
       " 0.08432536572217941,\n",
       " 1.6860214471817017,\n",
       " 2.2207348346710205,\n",
       " -1.5307567119598389,\n",
       " -2.04518461227417,\n",
       " -0.3002798557281494,\n",
       " 1.389941692352295,\n",
       " -0.3320818543434143,\n",
       " -0.7512526512145996,\n",
       " 0.040388625115156174,\n",
       " -0.05729524791240692,\n",
       " 0.9186842441558838,\n",
       " 1.926947832107544,\n",
       " 0.2793116569519043,\n",
       " -0.48924076557159424,\n",
       " 0.23466403782367706,\n",
       " 0.9163917303085327,\n",
       " -0.8370770812034607,\n",
       " 1.2657570838928223,\n",
       " -0.9519325494766235,\n",
       " -0.09683762490749359,\n",
       " 0.08898089826107025,\n",
       " -0.4768052101135254,\n",
       " 1.1513844728469849,\n",
       " -0.7403278350830078,\n",
       " -1.046450138092041,\n",
       " 0.6391959190368652,\n",
       " -0.9134833216667175,\n",
       " 0.30626586079597473,\n",
       " -0.4438096582889557,\n",
       " 1.5718194246292114,\n",
       " -2.5500645637512207,\n",
       " 0.0773279070854187,\n",
       " 0.4785814583301544,\n",
       " -0.13711243867874146,\n",
       " -1.6378331184387207,\n",
       " -0.5288330912590027,\n",
       " 0.49956005811691284,\n",
       " -0.14864154160022736,\n",
       " -0.10533568263053894,\n",
       " 0.03735391050577164,\n",
       " -0.038848504424095154,\n",
       " -0.4806070029735565,\n",
       " 0.025183429941534996,\n",
       " -0.6962173581123352,\n",
       " 0.4530291259288788,\n",
       " 0.7542916536331177,\n",
       " -2.7606170177459717,\n",
       " 1.3919645547866821,\n",
       " -1.0843313932418823,\n",
       " 1.0188275575637817,\n",
       " 1.2734277248382568,\n",
       " 1.4370743036270142,\n",
       " 0.4975038468837738,\n",
       " -0.9341806173324585,\n",
       " 0.5810812711715698,\n",
       " 1.195919394493103,\n",
       " -0.11540766060352325,\n",
       " 1.3854498863220215,\n",
       " 0.5413325428962708,\n",
       " 2.045870542526245,\n",
       " 0.31810784339904785,\n",
       " -1.9411506652832031,\n",
       " -1.0786772966384888,\n",
       " 0.07509475201368332,\n",
       " -0.1847476363182068,\n",
       " 4.7301764488220215,\n",
       " 0.689970076084137,\n",
       " 0.9828726649284363,\n",
       " 0.22608572244644165,\n",
       " 0.11635996401309967,\n",
       " -0.778649091720581,\n",
       " -0.6041231155395508,\n",
       " 0.25905129313468933,\n",
       " -1.5226738452911377,\n",
       " -0.08785279840230942,\n",
       " 0.5937318801879883,\n",
       " -1.445430874824524,\n",
       " 0.055293284356594086,\n",
       " 0.2710130214691162,\n",
       " 0.2124045193195343,\n",
       " -0.42308083176612854,\n",
       " 1.6041324138641357,\n",
       " 1.6035528182983398,\n",
       " -2.0751953125,\n",
       " -0.2659761607646942,\n",
       " 0.43637239933013916,\n",
       " 0.6455414891242981,\n",
       " 1.6862905025482178,\n",
       " -0.18064506351947784,\n",
       " -0.679349422454834,\n",
       " 1.3061429262161255,\n",
       " -0.9573458433151245,\n",
       " 1.0733680725097656,\n",
       " -0.2354806363582611,\n",
       " -1.252181053161621,\n",
       " -0.9836453795433044,\n",
       " -0.7147364616394043,\n",
       " -1.082034707069397,\n",
       " -0.7328023910522461,\n",
       " 1.6488250494003296,\n",
       " -0.17355704307556152,\n",
       " 2.4508090019226074,\n",
       " 0.7748342752456665,\n",
       " -0.2062470018863678,\n",
       " -0.3263375163078308,\n",
       " 0.6665350198745728,\n",
       " -0.9054286479949951,\n",
       " -0.27811408042907715,\n",
       " -0.821196436882019,\n",
       " -1.4834665060043335,\n",
       " -0.3969709873199463,\n",
       " 2.209873676300049,\n",
       " 0.8723143935203552,\n",
       " 0.0006066709756851196,\n",
       " 1.3357816934585571,\n",
       " 0.533513605594635,\n",
       " 1.7316778898239136,\n",
       " 0.38011956214904785,\n",
       " -0.5198136568069458,\n",
       " 1.1407580375671387,\n",
       " 0.11648739874362946,\n",
       " -1.336535096168518,\n",
       " 0.7289997339248657,\n",
       " -0.8430008888244629,\n",
       " -1.7587276697158813,\n",
       " 0.36434677243232727,\n",
       " -0.7766503095626831,\n",
       " -1.3209606409072876,\n",
       " -2.8082401752471924,\n",
       " -1.3143832683563232,\n",
       " -0.26054781675338745,\n",
       " -0.6219989061355591,\n",
       " -0.21372704207897186,\n",
       " 0.14000709354877472,\n",
       " -1.5270081758499146,\n",
       " -1.5467567443847656,\n",
       " -0.14869800209999084,\n",
       " -0.6039300560951233,\n",
       " -1.3102551698684692,\n",
       " -0.5923406481742859,\n",
       " 1.5125808715820312,\n",
       " -0.19291797280311584,\n",
       " 1.7666233777999878,\n",
       " ...]"
      ]
     },
     "execution_count": 633,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_decoded[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original...\n",
      "what do you call a muslim basketball player's best move islam dunk the the the \n",
      "Generated...\n",
      "why do hitler kill he because they come fish and bullets it looked realistic greatgrandchildren \n",
      "===========================\n",
      "Original...\n",
      "what's it called when a priest is always late collared people's time the the the \n",
      "Generated...\n",
      "where'd you get it the parrot said africa they have millions of them impeach ihop \n",
      "===========================\n",
      "Original...\n",
      "where does a snowman keep his money in the snowbank i'll see myself out the \n",
      "Generated...\n",
      "what kind of he is twentythird before he got married a governor biased subreddit attaches \n",
      "===========================\n",
      "Original...\n",
      "how often do you find a eunuch in westeros it varys the the the the \n",
      "Generated...\n",
      "why don't you grow them flies dog i'll let them go through see to greatgrandchildren \n",
      "===========================\n",
      "Original...\n",
      "what did the stop light say to the car don't look i am changing the \n",
      "Generated...\n",
      "what is a nephew quantity of sense that you'll never understand you arrow legend barb \n",
      "===========================\n",
      "Original...\n",
      "why did the apple cross the road to get to the other cider the the \n",
      "Generated...\n",
      "why is the sea salty because the land does not stick back himself 1972 greatgrandchildren \n",
      "===========================\n",
      "Original...\n",
      "what did the red dog say to the blue dog nothing dogs can't speak the \n",
      "Generated...\n",
      "what's the difference between arabs and women dogs faggot mouth they shit suck pleases scrubbed \n",
      "===========================\n",
      "Original...\n",
      "what happens to nitrogen when the sun rises it becomes daytrogen the the the the \n",
      "Generated...\n",
      "why do men like clothes day they always sneak back on the families squints ft \n",
      "===========================\n",
      "Original...\n",
      "what did the cat say to the person nothing because cats don't speak the the \n",
      "Generated...\n",
      "what do you call a cow with no legs my nose capricorn sister alan namibia \n",
      "===========================\n",
      "Original...\n",
      "what did the setup say to the punchline you're nothing but a joke the the \n",
      "Generated...\n",
      "how do you know i feel i have to walk out alone pilgrim myself greatgrandchildren \n",
      "===========================\n"
     ]
    }
   ],
   "source": [
    "for i, sentence in enumerate(sent_decoded):\n",
    "    print(\"Original...\")\n",
    "    print_sentence_with_w2v(help_vectorized_test[i])\n",
    "    print(\"Generated...\")\n",
    "    print_sentence_with_w2v(sentence)\n",
    "    print(\"===========================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder trained above embeds sentences (concatenated word vetors) into a lower dimensional space. The code below takes two of these lower dimensional sentence representations and finds five points between them. It then uses the trained decoder to project these five points into the higher, original, dimensional space. Finally, it reveals the text represented by the five generated sentence vectors by taking each word vector concatenated inside and finding the text associated with it in the word2vec used during preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_hom = shortest_homology(sent_encoded[3], sent_encoded[10], 5)\n",
    "for point in test_hom:\n",
    "    p = generator.predict(np.array([point]))[0]\n",
    "    print_sentence_with_w2v(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below does the same thing, with one important difference. After sampling equidistant points in the latent space between two sentence embeddings, it finds the embeddings from our encoded dataset those points are most similar to. It then prints the text associated with those vectors.\n",
    "  \n",
    "This allows us to explore how the Variational Autoencoder clusters our dataset of sentences in latent space. It lets us investigate whether sentences with similar concepts or grammatical styles are represented in similar areas of the lower dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_hom = shortest_homology(sent_encoded[2], sent_encoded[1500], 20)\n",
    "for point in test_hom:\n",
    "    p = generator.predict(np.array([find_similar_encoding(point)]))[0]\n",
    "    print_sentence_with_w2v(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Network"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
