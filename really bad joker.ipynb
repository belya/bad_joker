{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import itertools\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "from scipy.stats import norm\n",
    "import nltk.data\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import reuters\n",
    "from nltk. corpus import gutenberg\n",
    "from nltk.corpus import brown\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.layers import Input, Dense, Lambda, Layer, LSTM, Reshape, TimeDistributed, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing code is data specific.  \n",
    "  \n",
    "It is an example of how one can use a pre-trained word2vec to embed sentences into a vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO train w2v model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"jokes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sentences = dataset[\"Answer\"].tolist()\n",
    "y_sentences = dataset[\"Question\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    return re.sub(\n",
    "        r\"[^\\w\\s]\", \n",
    "        \"\", \n",
    "        text\n",
    "    ).lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_x_sentences = [preprocess_text(t) for t in x_sentences]\n",
    "preprocessed_y_sentences = [preprocess_text(t) for t in y_sentences]\n",
    "preprocessed_sentences = preprocessed_y_sentences + preprocessed_x_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(preprocessed_sentences, size=100, window=10, workers=4, iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('trumps', 0.6275929808616638),\n",
       " ('hillary', 0.5711350440979004),\n",
       " ('sanders', 0.4482596814632416),\n",
       " ('bernie', 0.4478847086429596),\n",
       " ('dog', 0.4161553978919983),\n",
       " ('presidential', 0.4067925214767456),\n",
       " ('hilary', 0.3990013003349304),\n",
       " ('clinton', 0.39776691794395447),\n",
       " ('showing', 0.39756664633750916),\n",
       " ('goofy', 0.39537447690963745)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"trump\"], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = w2v_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sentence(sentence):\n",
    "    concat_vector = []\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            concat_vector.append(w2v[word])\n",
    "        except:\n",
    "            pass\n",
    "    return [a for vector in concat_vector for a in vector]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing text from a variety of different sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vectorized = [vectorize_sentence(sentence) for sentence in preprocessed_x_sentences]\n",
    "y_vectorized = [vectorize_sentence(sentence) for sentence in preprocessed_y_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dim = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "x_vectorized_padded = sequence.pad_sequences(x_vectorized, maxlen=original_dim, padding=\"post\", truncating=\"post\")\n",
    "y_vectorized_padded = sequence.pad_sequences(y_vectorized, maxlen=original_dim, padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to shuffle the text vectors before splitting them into test and train samples.   \n",
    "  \n",
    "This is done to avoid clumping text with similar context and style in the dataset because it can confuse the neural network during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vectorized_train, x_vectorized_test, y_vectorized_train, y_vectorized_test = train_test_split(x_vectorized_padded, y_vectorized_padded, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "def cut_dataset(dataset, batch_size):\n",
    "    rest = len(dataset) % batch_size\n",
    "    return dataset[:-rest]\n",
    "\n",
    "x_vectorized_train = cut_dataset(x_vectorized_train, batch_size)\n",
    "y_vectorized_train = cut_dataset(y_vectorized_train, batch_size)\n",
    "x_vectorized_test = cut_dataset(x_vectorized_test, batch_size)\n",
    "y_vectorized_test = cut_dataset(y_vectorized_test, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get w2v embeddings for text with fixed length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 1000\n",
    "intermediate_dim = 1200\n",
    "lstm_intermediate_dim = 100\n",
    "epochs = 200\n",
    "epsilon_std = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Input(shape=(original_dim,))\n",
    "h = Dense(intermediate_dim, activation='sigmoid')(x)\n",
    "z_mean = Dense(latent_dim)(h)\n",
    "z_log_var = Dense(latent_dim)(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0.,\n",
    "                              stddev=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we instantiate these layers separately so as to reuse them later\n",
    "decoder_h = Dense(intermediate_dim, activation='sigmoid')\n",
    "decoder_mean = Dense(original_dim, activation='sigmoid')\n",
    "h_decoded = decoder_h(z)\n",
    "x_decoded_mean = decoder_mean(h_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(y_true, y_pred):\n",
    "    xent_loss = K.sum(K.binary_crossentropy(y_pred, y_true), axis=-1)\n",
    "    kl_loss = 0.5 * K.sum(K.square(z_mean) + K.exp(z_log_var) - 1. - z_log_var, axis=-1)\n",
    "    return xent_loss + kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = Model(x, x_decoded_mean)\n",
    "vae.compile(optimizer='adam', loss=vae_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26600 samples, validate on 11400 samples\n",
      "Epoch 1/200\n",
      "26600/26600 [==============================] - 28s 1ms/step - loss: 2332.4864 - val_loss: 2293.3803\n",
      "\n",
      "Epoch 00001: saving model to /tmp/model.h5\n",
      "Epoch 2/200\n",
      "26600/26600 [==============================] - 28s 1ms/step - loss: 2187.4393 - val_loss: 2150.3067\n",
      "\n",
      "Epoch 00002: saving model to /tmp/model.h5\n",
      "Epoch 3/200\n",
      "26600/26600 [==============================] - 29s 1ms/step - loss: 2076.9054 - val_loss: 2079.0410\n",
      "\n",
      "Epoch 00003: saving model to /tmp/model.h5\n",
      "Epoch 4/200\n",
      "26600/26600 [==============================] - 28s 1ms/step - loss: 2053.8142 - val_loss: 2073.4971\n",
      "\n",
      "Epoch 00004: saving model to /tmp/model.h5\n",
      "Epoch 5/200\n",
      "26600/26600 [==============================] - 31s 1ms/step - loss: 2050.7585 - val_loss: 2070.5589\n",
      "\n",
      "Epoch 00005: saving model to /tmp/model.h5\n",
      "Epoch 6/200\n",
      "26600/26600 [==============================] - 27s 1ms/step - loss: 2048.0073 - val_loss: 2070.2853\n",
      "\n",
      "Epoch 00006: saving model to /tmp/model.h5\n",
      "Epoch 7/200\n",
      "26600/26600 [==============================] - 29s 1ms/step - loss: 2047.7457 - val_loss: 2070.0404\n",
      "\n",
      "Epoch 00007: saving model to /tmp/model.h5\n",
      "Epoch 8/200\n",
      "26600/26600 [==============================] - 29s 1ms/step - loss: 2047.5351 - val_loss: 2069.8315\n",
      "\n",
      "Epoch 00008: saving model to /tmp/model.h5\n",
      "Epoch 9/200\n",
      "26600/26600 [==============================] - 29s 1ms/step - loss: 2047.3630 - val_loss: 2069.6721\n",
      "\n",
      "Epoch 00009: saving model to /tmp/model.h5\n",
      "Epoch 10/200\n",
      "26600/26600 [==============================] - 29s 1ms/step - loss: 2047.2200 - val_loss: 2069.5441\n",
      "\n",
      "Epoch 00010: saving model to /tmp/model.h5\n",
      "Epoch 11/200\n",
      "26600/26600 [==============================] - 29s 1ms/step - loss: 2047.0950 - val_loss: 2069.4424\n",
      "\n",
      "Epoch 00011: saving model to /tmp/model.h5\n",
      "Epoch 12/200\n",
      "26600/26600 [==============================] - 29s 1ms/step - loss: 2046.9934 - val_loss: 2069.3393\n",
      "\n",
      "Epoch 00012: saving model to /tmp/model.h5\n",
      "Epoch 13/200\n",
      "26600/26600 [==============================] - 29s 1ms/step - loss: 2046.9104 - val_loss: 2069.2691\n",
      "\n",
      "Epoch 00013: saving model to /tmp/model.h5\n",
      "Epoch 14/200\n",
      "26600/26600 [==============================] - 29s 1ms/step - loss: 2046.8462 - val_loss: 2069.2157\n",
      "\n",
      "Epoch 00014: saving model to /tmp/model.h5\n",
      "Epoch 15/200\n",
      "26600/26600 [==============================] - 29s 1ms/step - loss: 2046.7876 - val_loss: 2069.1720\n",
      "\n",
      "Epoch 00015: saving model to /tmp/model.h5\n",
      "Epoch 16/200\n",
      "26600/26600 [==============================] - 29s 1ms/step - loss: 2046.7528 - val_loss: 2069.1274\n",
      "\n",
      "Epoch 00016: saving model to /tmp/model.h5\n",
      "Epoch 17/200\n",
      "26600/26600 [==============================] - 29s 1ms/step - loss: 2046.7205 - val_loss: 2069.0953\n",
      "\n",
      "Epoch 00017: saving model to /tmp/model.h5\n",
      "Epoch 18/200\n",
      "26600/26600 [==============================] - 29s 1ms/step - loss: 2046.6929 - val_loss: 2069.0721\n",
      "\n",
      "Epoch 00018: saving model to /tmp/model.h5\n",
      "Epoch 19/200\n",
      "26600/26600 [==============================] - 29s 1ms/step - loss: 2046.6714 - val_loss: 2069.0534\n",
      "\n",
      "Epoch 00019: saving model to /tmp/model.h5\n",
      "Epoch 20/200\n",
      "26600/26600 [==============================] - 29s 1ms/step - loss: 2046.6558 - val_loss: 2069.0342\n",
      "\n",
      "Epoch 00020: saving model to /tmp/model.h5\n",
      "Epoch 21/200\n",
      "26600/26600 [==============================] - 29s 1ms/step - loss: 2046.6391 - val_loss: 2069.0203\n",
      "\n",
      "Epoch 00021: saving model to /tmp/model.h5\n",
      "Epoch 22/200\n",
      "26600/26600 [==============================] - 29s 1ms/step - loss: 2046.6272 - val_loss: 2069.0046\n",
      "\n",
      "Epoch 00022: saving model to /tmp/model.h5\n",
      "Epoch 23/200\n",
      "26600/26600 [==============================] - 29s 1ms/step - loss: 2046.6156 - val_loss: 2068.9972\n",
      "\n",
      "Epoch 00023: saving model to /tmp/model.h5\n",
      "Epoch 24/200\n",
      "26600/26600 [==============================] - 29s 1ms/step - loss: 2046.6087 - val_loss: 2068.9866\n",
      "\n",
      "Epoch 00024: saving model to /tmp/model.h5\n",
      "Epoch 25/200\n",
      "26600/26600 [==============================] - 29s 1ms/step - loss: 2046.5999 - val_loss: 2068.9884\n",
      "\n",
      "Epoch 00025: saving model to /tmp/model.h5\n",
      "Epoch 26/200\n",
      "26600/26600 [==============================] - 29s 1ms/step - loss: 2046.5988 - val_loss: 2068.9751\n",
      "\n",
      "Epoch 00026: saving model to /tmp/model.h5\n",
      "Epoch 27/200\n",
      "26600/26600 [==============================] - 29s 1ms/step - loss: 2046.5846 - val_loss: 2068.9676\n",
      "\n",
      "Epoch 00027: saving model to /tmp/model.h5\n",
      "Epoch 28/200\n",
      "26600/26600 [==============================] - 29s 1ms/step - loss: 2046.5784 - val_loss: 2068.9608\n",
      "\n",
      "Epoch 00028: saving model to /tmp/model.h5\n",
      "Epoch 29/200\n",
      "26600/26600 [==============================] - 29s 1ms/step - loss: 2046.5770 - val_loss: 2068.9618\n",
      "\n",
      "Epoch 00029: saving model to /tmp/model.h5\n",
      "Epoch 30/200\n",
      "26600/26600 [==============================] - 29s 1ms/step - loss: 2046.5873 - val_loss: 2068.9769\n",
      "\n",
      "Epoch 00030: saving model to /tmp/model.h5\n",
      "Epoch 31/200\n",
      "26600/26600 [==============================] - 29s 1ms/step - loss: 2046.5860 - val_loss: 2068.9836\n",
      "\n",
      "Epoch 00031: saving model to /tmp/model.h5\n",
      "Epoch 32/200\n",
      "26600/26600 [==============================] - 29s 1ms/step - loss: 2046.5739 - val_loss: 2068.9432\n",
      "\n",
      "Epoch 00032: saving model to /tmp/model.h5\n",
      "Epoch 33/200\n",
      "26600/26600 [==============================] - 29s 1ms/step - loss: 2046.5711 - val_loss: 2068.9495\n",
      "\n",
      "Epoch 00033: saving model to /tmp/model.h5\n",
      "Epoch 34/200\n",
      "26600/26600 [==============================] - 29s 1ms/step - loss: 2046.5868 - val_loss: 2068.9565\n",
      "\n",
      "Epoch 00034: saving model to /tmp/model.h5\n",
      "Epoch 35/200\n",
      "26600/26600 [==============================] - 29s 1ms/step - loss: 2046.5681 - val_loss: 2068.9433\n",
      "\n",
      "Epoch 00035: saving model to /tmp/model.h5\n",
      "Epoch 36/200\n",
      "26600/26600 [==============================] - 29s 1ms/step - loss: 2046.5811 - val_loss: 2068.9367\n",
      "\n",
      "Epoch 00036: saving model to /tmp/model.h5\n",
      "Epoch 37/200\n",
      "26600/26600 [==============================] - 29s 1ms/step - loss: 2046.5776 - val_loss: 2069.0225\n",
      "\n",
      "Epoch 00037: saving model to /tmp/model.h5\n",
      "Epoch 38/200\n",
      "26600/26600 [==============================] - 29s 1ms/step - loss: 2046.5869 - val_loss: 2068.9429\n",
      "\n",
      "Epoch 00038: saving model to /tmp/model.h5\n",
      "Epoch 39/200\n",
      "26600/26600 [==============================] - 29s 1ms/step - loss: 2046.6032 - val_loss: 2068.9284\n",
      "\n",
      "Epoch 00039: saving model to /tmp/model.h5\n",
      "Epoch 40/200\n",
      "26600/26600 [==============================] - 29s 1ms/step - loss: 2046.5833 - val_loss: 2068.9567\n",
      "\n",
      "Epoch 00040: saving model to /tmp/model.h5\n",
      "Epoch 41/200\n",
      "26600/26600 [==============================] - 29s 1ms/step - loss: 2046.5757 - val_loss: 2068.9307\n",
      "\n",
      "Epoch 00041: saving model to /tmp/model.h5\n",
      "Epoch 42/200\n",
      "26600/26600 [==============================] - 29s 1ms/step - loss: 2046.5763 - val_loss: 2068.9373\n",
      "\n",
      "Epoch 00042: saving model to /tmp/model.h5\n",
      "Epoch 43/200\n",
      "26600/26600 [==============================] - 29s 1ms/step - loss: 2046.5845 - val_loss: 2068.9409\n",
      "\n",
      "Epoch 00043: saving model to /tmp/model.h5\n",
      "Epoch 44/200\n",
      "26600/26600 [==============================] - 29s 1ms/step - loss: 2046.5817 - val_loss: 2068.9542\n",
      "\n",
      "Epoch 00044: saving model to /tmp/model.h5\n",
      "Epoch 45/200\n",
      "26600/26600 [==============================] - 29s 1ms/step - loss: 2046.6056 - val_loss: 2069.0275\n",
      "\n",
      "Epoch 00045: saving model to /tmp/model.h5\n",
      "Epoch 46/200\n",
      "26600/26600 [==============================] - 29s 1ms/step - loss: 2046.5962 - val_loss: 2068.9374\n",
      "\n",
      "Epoch 00046: saving model to /tmp/model.h5\n",
      "Epoch 47/200\n",
      "26600/26600 [==============================] - 29s 1ms/step - loss: 2046.5890 - val_loss: 2068.9412\n",
      "\n",
      "Epoch 00047: saving model to /tmp/model.h5\n",
      "Epoch 48/200\n",
      "26600/26600 [==============================] - 29s 1ms/step - loss: 2046.5666 - val_loss: 2068.9534\n",
      "\n",
      "Epoch 00048: saving model to /tmp/model.h5\n",
      "Epoch 49/200\n",
      "26600/26600 [==============================] - 30s 1ms/step - loss: 2046.5811 - val_loss: 2068.9411\n",
      "\n",
      "Epoch 00049: saving model to /tmp/model.h5\n",
      "Epoch 50/200\n",
      "13000/26600 [=============>................] - ETA: 14s - loss: 2057.7235"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-33413d0ff6ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_vectorized_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_vectorized_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         callbacks=cp)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#checkpoint\n",
    "cp = [callbacks.ModelCheckpoint(filepath=\"/tmp/model.h5\", verbose=1)]\n",
    "\n",
    "#train\n",
    "vae.fit(x_vectorized_train, x_vectorized_train,\n",
    "        shuffle=True,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(x_vectorized_test, x_vectorized_test),\n",
    "        callbacks=cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a model to project inputs on the latent space\n",
    "encoder = Model(x, z_mean)\n",
    "\n",
    "# build a generator that can sample from the learned distribution\n",
    "decoder_input = Input(shape=(latent_dim,))\n",
    "_h_decoded = decoder_h(decoder_input)\n",
    "_x_decoded_mean = decoder_mean(_h_decoded)\n",
    "generator = Model(decoder_input, _x_decoded_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Text From Latent Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some matrix magic\n",
    "def sent_parse(sentence, mat_shape):\n",
    "    data_concat = []\n",
    "    word_vecs = vectorize_sentences(sentence)\n",
    "    for x in word_vecs:\n",
    "        data_concat.append(list(itertools.chain.from_iterable(x)))\n",
    "    zero_matr = np.zeros(mat_shape)\n",
    "    zero_matr[0] = np.array(data_concat)\n",
    "    return zero_matr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sentence_with_w2v(sent_vect):\n",
    "    word_sent = ''\n",
    "    tocut = sent_vect\n",
    "    for i in range (int(len(sent_vect)/100)):\n",
    "        word_sent += w2v.most_similar(positive=[tocut[:100]], topn=1)[0][0]\n",
    "        word_sent += ' '\n",
    "        tocut = tocut[100:]\n",
    "    print(word_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input: encoded sentence vector\n",
    "# output: encoded sentence vector in dataset with highest cosine similarity\n",
    "def find_similar_encoding(sent_vect):\n",
    "    all_cosine = []\n",
    "    for sent in sent_encoded:\n",
    "        result = 1 - spatial.distance.cosine(sent_vect, sent)\n",
    "        all_cosine.append(result)\n",
    "    data_array = np.array(all_cosine)\n",
    "    maximum = data_array.argsort()[-3:][::-1][1]\n",
    "    new_vec = sent_encoded[maximum]\n",
    "    return new_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input: two points, integer n\n",
    "# output: n equidistant points on the line between the input points (inclusive)\n",
    "def shortest_homology(point_one, point_two, num):\n",
    "    dist_vec = point_two - point_one\n",
    "    sample = np.linspace(0, 1, num, endpoint = True)\n",
    "    hom_sample = []\n",
    "    for s in sample:\n",
    "        hom_sample.append(point_one + s * dist_vec)\n",
    "    return hom_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input: two written sentences, VAE batch-size, dimension of VAE input\n",
    "# output: the function embeds the sentences in latent-space, and then prints their generated text representations\n",
    "# along with the text representations of several points in between them\n",
    "def sent_2_sent(sent1,sent2, batch, dim):\n",
    "    a = sent_parse([sent1], (batch,dim))\n",
    "    b = sent_parse([sent2], (batch,dim))\n",
    "    encode_a = encoder.predict(a, batch_size = batch)\n",
    "    encode_b = encoder.predict(b, batch_size = batch)\n",
    "    test_hom = hom_shortest(encode_a[0], encode_b[0], 5)\n",
    "    \n",
    "    for point in test_hom:\n",
    "        p = generator.predict(np.array([point]))[0]\n",
    "        print_sentence(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing sentences from the training set and comparing them with the original will test whether the custom print function works properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', 'nearly', 'drown', 'in', 'his', 'own', 'tea', 'pee']\n",
      "he nearly drown in his own tea pee the the the the the the the the the the the the \n",
      "['mycheexarphlexin']\n",
      "the the the the the the the the the the the the the the the the the the the the \n",
      "['matt']\n",
      "matt the the the the the the the the the the the the the the the the the the the \n",
      "['jeanluc', 'pickacard']\n",
      "the the the the the the the the the the the the the the the the the the the the \n",
      "['a', 'bullet', 'doesnt', 'miss', 'harambe']\n",
      "a bullet doesnt miss harambe the the the the the the the the the the the the the the the \n",
      "['he', 'was', 'having', 'a', 'midlife', 'crisis']\n",
      "he was having a midlife crisis the the the the the the the the the the the the the the \n",
      "['one', 'shucks', 'between', 'fits']\n",
      "one shucks between fits the the the the the the the the the the the the the the the the \n",
      "['kevin', 'durant', 'or', 'bernie', 'sanders']\n",
      "kevin or bernie sanders the the the the the the the the the the the the the the the the \n",
      "['because', 'the', 'shark', 'burped']\n",
      "because the shark the the the the the the the the the the the the the the the the the \n",
      "['a', 'bachelor', 'will', 'go', 'to', 'the', 'fridge', 'sees', 'nothing', 'he', 'wants', 'and', 'go', 'to', 'bed', 'a', 'married', 'man', 'will', 'go', 'the', 'bed', 'sees', 'nothing', 'he', 'wants', 'and', 'go', 'the', 'fridge']\n",
      "a will go to the fridge sees nothing he wants and go to bed a married man will go the \n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 10):\n",
    "    print(preprocessed_x_sentences[i])\n",
    "    print_sentence_with_w2v(x_vectorized_padded[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder takes the training set of sentence vectors (concatenanted word vectors) and embeds them into a lower dimensional vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_encoded = encoder.predict(x_vectorized_padded[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder takes the list of latent dimensional encodings from above and turns them back into vectors of their original dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_decoded = generator.predict(sent_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brides equal purchase manage purchase purchase purchase purchase purchase um purchase purchase purchase purchase purchase purchase purchase purchase purchase purchase \n"
     ]
    }
   ],
   "source": [
    "print_sentence_with_w2v(sent_decoded[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder trained above embeds sentences (concatenated word vetors) into a lower dimensional space. The code below takes two of these lower dimensional sentence representations and finds five points between them. It then uses the trained decoder to project these five points into the higher, original, dimensional space. Finally, it reveals the text represented by the five generated sentence vectors by taking each word vector concatenated inside and finding the text associated with it in the word2vec used during preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_hom = shortest_homology(sent_encoded[3], sent_encoded[10], 5)\n",
    "for point in test_hom:\n",
    "    p = generator.predict(np.array([point]))[0]\n",
    "    print_sentence_with_w2v(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below does the same thing, with one important difference. After sampling equidistant points in the latent space between two sentence embeddings, it finds the embeddings from our encoded dataset those points are most similar to. It then prints the text associated with those vectors.\n",
    "  \n",
    "This allows us to explore how the Variational Autoencoder clusters our dataset of sentences in latent space. It lets us investigate whether sentences with similar concepts or grammatical styles are represented in similar areas of the lower dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_hom = shortest_homology(sent_encoded[2], sent_encoded[1500], 20)\n",
    "for point in test_hom:\n",
    "    p = generator.predict(np.array([find_similar_encoding(point)]))[0]\n",
    "    print_sentence_with_w2v(p)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
